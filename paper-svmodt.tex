% !TeX root = RJwrapper.tex
\title{svmodt: An R Package for Linear SVM-Based Oblique Decision Trees}


\author{by Aneesh Agarwal, Jack Jewson, and Erik Sverdrup}

\maketitle

\abstract{%
Decision trees are widely used for classification tasks due to their interpretability, but traditional axis-aligned splits often require deep trees to approximate complex decision boundaries. We introduce svmodt, an R package implementing two an SVM-based oblique decision tree (SVMODT) that uses linear Support Vector Machines at each node to create multivariate splits. The package supports dynamic feature selection strategies, node-specific class weighting for handling imbalanced data, and feature diversity mechanisms through penalization. We demonstrate that SVMODT achieves competitive accuracy with standard SVMs while maintaining the interpretability advantages of tree structures. The package includes comprehensive visualization tools, detailed documentation, and is freely available on GitHub.
}

\section{Introduction}\label{introduction}

Decision trees remain widely used in machine learning because they are interpretable, straightforward to apply, and can accommodate both categorical and numerical predictors with minimal preprocessing. Classical algorithms such as CART \citep{breiman1984} and C4.5 \citep{quinlan1993} employ axis-aligned (univariate) splits that partition the feature space along coordinate axes, thereby preserving transparency in the resulting decision rules. In R, traditional decision trees are readily implemented via packages such as \CRANpkg{rpart}, \CRANpkg{tree}, and \CRANpkg{party}. Although axis-parallel decision trees offer high interpretability, they suffer representational limitations: they frequently grow unnecessarily large as they approximate complex relationships via multiple axis-aligned partitions, which can lead to replicated subtrees and repeated testing of the same feature along different paths, reducing efficiency and impairing generalization \citep{pagallo1990}. Oblique decision trees mitigate these limitations by permitting splits on linear combinations of features; for example, a single oblique test of the form \(w_1x_1 + w_2x_2 < \theta\) can often replace multiple axis-aligned tests, producing more compact and geometrically simpler partitions of the feature space. Oblique trees are available in R through packages such as \CRANpkg{ODRF}, \CRANpkg{aorsf}, and \CRANpkg{oblique.tree}. Empirical comparisons across diverse data sets indicate that oblique decision trees typically attain higher accuracy and smaller tree sizes than their axis-parallel counterparts, albeit at the expense of some interpretability. Importantly, oblique decision trees preserve key advantages of standard trees---sequential split evaluation and transparent decision procedures---while providing greater modeling flexibility for complex data sets \citep{kozial2009, friedl1997, huan1998, canete}.

\begin{figure}

{\centering \subfloat[axis-parallel split\label{fig:palmer-penguins-introduction-split-1}]{\includegraphics[width=0.5\linewidth]{paper-svmodt_files/figure-latex/palmer-penguins-introduction-split-1} }\subfloat[axis-oblique split\label{fig:palmer-penguins-introduction-split-2}]{\includegraphics[width=0.5\linewidth]{paper-svmodt_files/figure-latex/palmer-penguins-introduction-split-2} }

}

\caption{Comparison of Axis-parallel and Axis-oblique splits on Palmerpenguins dataset.}\label{fig:palmer-penguins-introduction-split}
\end{figure}

Figure \ref{fig:palmer-penguins-introduction-split} contrasts an axis-parallel decision tree with an axis-oblique decision tree. The axis-parallel tree displays (left panel) clear overfitting and a high rate of training misclassification, reflecting its inability to capture interactions among predictors that determine class membership. In contrast, the oblique tree (right panel) yields geometrically simpler decision boundaries and substantially fewer misclassifications on the training set, indicating a better fit to the underlying multivariate relationships. Together, these panels illustrate how oblique splits can more effectively represent linear combinations of features and thereby reduce spurious complexity and error in tree-based classifiers.

Support Vector Machines (SVMs), introduced by \citet{cortes1995}, provide a principled way to obtain oblique splits by finding margin‑maximizing hyperplanes. The idea of fitting SVMs at tree nodes was formalized by \citet{bennet1998} and extended to multiclass settings and other refinements in subsequent work \citep{takahashi2002, bala2011, ganaie2022}.

Python's STree implements SVM‑node oblique trees using scikit‑learn's SVC \citep{Montanana2021, scikit2011}, while Java tools such as Weka offer components that can be adapted for hierarchical SVM trees \citep{menkovski2008}; however, fully featured, native R implementations remain limited. To address this gap we make two contributions. First, we provide an R implementation of an STree‑style oblique tree that does not call Python, delivering a production‑ready API and efficient, vectorized operations for R users. Second, we propose SVM‑ODT, an enhanced SVM‑based oblique decision tree that extends the STree algorithm with practical improvements which include feature sub-setting, feature selection, feature penalization, node-specific class weighting to improve performance and robustness in both binary and multiclass settings. Together these contributions aim to bring the representational advantages of SVM‑based oblique splits to R practitioners while addressing key computational and methodological limitations of existing approaches. We are exclusively going to focus on building a supervised learning algorithm capable of predicitng \(y\) from d-dimensional predictors \(x\).

To address this methodological gap, we advance two primary contributions.\\
First, we introduce a fully native R implementation of an STree‑style oblique decision tree that operates independently of Python. This implementation provides a production‑ready API and leverages vectorized computation to ensure efficiency and seamless integration within the R ecosystem.

Second, we develop \textbf{SVM‑ODT}, an enhanced SVM‑based oblique decision tree that extends the original STree framework through several practical innovations. These include feature sub‑setting, embedded feature selection, feature penalization, and node‑specific class weighting. Collectively, these enhancements are designed to improve predictive performance, interpretability, and robustness across both binary and multiclass classification settings.

Together, these contributions aim to make the representational strengths of SVM‑driven oblique splits accessible to R programmers while simultaneously addressing key computational and methodological limitations present in existing approaches. Throughout this work, we focus exclusively on the supervised learning problem of predicting \(y\) from \(d\)-dimensional predictors \(x\), where the goal is to learn a decision function that maps \(x \in \mathbb{R}^d \quad \longrightarrow \quad y\).

\section{Background}\label{background}

\subsection{Decision Trees}\label{decision-trees}

Decision Trees (DTs) are interpretable classification models that represent their decision-making process through a hierarchical, tree-like structure. This structure comprises of internal nodes containing splitting criteria and terminal (leaf) nodes corresponding to predicted outcomes (class labels and probabilities). The nodes are connected by directed edges, each representing a possible outcome of a splitting criterion. Formally, a DT can be expressed as a rooted, directed tree \(T = (G(V, E), v_1)\), where \(V\) denotes the set of nodes, \(E\) represents the set of edges linking these nodes, and \(v_1\) is the root node.

If the tree \(T\) has \(m\) nodes, then for any \(j \in \{1, \ldots, m\}\), the set of child nodes of \(v_j \in V\) can be defined as:

\[
N^{+}(v_j) = \{ v_k \in V \mid k \in \{1, \ldots, m\},\; k \neq j,\; (v_j, v_k) \in E \}.
\]

Here, \(N^{+}(v_j)\) denotes the set of nodes that are directly connected to \(v_j\) through outgoing edges, representing two subsequent child nodes that can be reached from \(v_j\) within the tree structure \citep{lopez2018}.

Decision tree algorithms can be categorized based on whether the same type of test is applied at all internal nodes. \textbf{Homogeneous trees} employ a single algorithm throughout (e.g., univariate or multivariate splits), whereas \textbf{hybrid trees} allow different algorithms such as linear discriminant functions, \(k\)-nearest neighbors, or univariate splits that can be used in different sub-trees \citep{brodley1995}. Hybrid trees exploit the principle of \emph{selective superiority}, allowing subsets of the data to be modeled by the most appropriate classifier, thereby improving flexibility and accuracy.

\subsubsection{Axis-Parallel Decision Trees}\label{axis-parallel-decision-trees}

Axis-Parallel Decision trees represent hyper-planes dividing the instance space into several disjoint axis-parallel regions. Axis-parallel decision trees, such as CART and C4.5, represent two of the most widely used algorithms for classification tasks. The \textbf{CART (Classification and Regression Trees)} algorithm employs a binary recursive partitioning procedure at capable of handling both continuous and categorical variables as predictors or targets. At each node, the algorithm systematically evaluates every available variable and its potential split points to determine the optimal partition. In R, traditional decision trees are readily implemented via packages such as \CRANpkg{rpart}, \CRANpkg{tree}, and \CRANpkg{party}.

In contrast, \textbf{C4.5}, an extension of the earlier \textbf{ID3} algorithm \citep{quinlan1986}, utilizing information theory measures such as \textbf{information gain} and \textbf{gain ratio} to select the most informative attribute for each split \citep{quinlan1993}. C4.5 also includes mechanisms to handle missing attribute values by weighting instances according to the proportion of known data and employs an \textbf{error-based pruning} method to reduce overfitting. Although these techniques are effective across diverse data sets, studies have shown that the choice of pruning strategy and stopping criteria can significantly affect model performance across different domains \citep{mingers1989, schaffer1992}.

While axis-parallel decision tree are highly interpretability, they have several representational limitations. Such trees often grow unnecessarily large, as they must approximate complex relationship between features through multiple axis-aligned partitions. This can result in the replication of sub-trees and repeated testing of the same feature along different paths, both of which reduce efficiency and hinder generalization performance \citep{pagallo1990}.

\subsubsection{Axis-Oblique Decision Trees}\label{axis-oblique-decision-trees}

Axis-oblique decision trees extends axis-parallel decision trees by allowing each internal node to perform splits based on linear or nonlinear combinations \citep{said2025} of multiple features. This flexibility enables the tree to form oblique decision boundaries that more accurately partition the instance space. For example, a single multivariate test such as \(x + y < 8\) can replace multiple univariate splits needed to approximate the same boundary. Oblique trees are available in R through packages such as \CRANpkg{ODRF}, \CRANpkg{aorsf}, and \CRANpkg{oblique.tree}.

The construction of Axis-oblique decision trees introduces several design considerations, including how to represent multivariate tests, determine their coefficients, select features to include, handle symbolic and missing data, and prune to avoid over-fitting \citep{brodley1995}. Various optimization algorithms such as recursive least squares \citep{young1984}, the pocket algorithm \citep{gallant1986}, or thermal training \citep{frean1990} may be used to estimate the weights. However, Axis-oblique decision trees trade interpretability for representational power and often require additional mechanisms for \textbf{local feature selection}, such as \emph{sequential forward selection} (SFS) or \emph{sequential backward elimination} (SBE) \citep{kittler1986}.

Empirical comparisons across multiple data sets demonstrate that multivariate trees generally achieve higher accuracy and smaller tree sizes than their univariate counterparts, though this comes at the cost of reduced interpretability. Moreover, MDTs retain key advantages of standard decision trees such as sequential split criteria evaluation and transparent decision procedures while offering improved modeling flexibility for complex data sets \citep{kozial2009, friedl1997, huan1998, canete}.

\subsection{Support Vector Machines (SVMs)}\label{support-vector-machines-svms}

Support Vector Machines (SVMs) are powerful supervised learning models used for classification and regression tasks. They aim to determine an optimal separating hyperplane that maximizes the margin between binary classes in the data. This margin-based approach enhances the generalization ability of the model, making SVMs robust and effective for many real-world problems \citep{cristianini2000}. A simplest \textbf{linear SVMs} construct a separating hyperplane in an \(p\)-dimensional space such that the margin between the classes is maximized. Given a training dataset \(\{(\mathbf{x}_i, y_i)\}_{i=1}^{N}\), where \(\mathbf{x}_i \in \mathbb{R}^p\) and \(y_i \in \{-1, +1\}\), the decision function is defined as \(f(\mathbf{x}) = \text{sign}(\mathbf{w}^\top \mathbf{x} + b)\). The optimal hyperplane is the one that maximizes the distance between the closest points of each class (the \textbf{support vectors}) and the hyperplane itself \citep{cortes1995}. While linear classifiers provide useful insights, they are often inadequate for real-world data sets, where classes are not linearly separable. In such cases, SVMs can be extended to create \textbf{nonlinear decision boundaries} by mapping the input vectors into a higher-dimensional \textbf{feature space} using a nonlinear transformation \(\phi: \mathbb{R}^n \rightarrow \mathcal{F}\). The linear transformation is then achieved in the transformed space using \(f(\mathbf{x}) = \text{sign}(\mathbf{w}^\top \phi(\mathbf{x}) + b)\).

Although SVMs exhibit strong theoretical foundations and robust generalization capabilities, they present several practical limitations. Model performance is highly dependent on the appropriate selection of hyperparameters such as the regularization term (\(C\)) and kernel parameters (e.g., \(\gamma\)), which govern the trade-off between margin maximization and misclassification tolerance \citep{nanda2018}. Training an SVM requires solving a quadratic programming (QP) optimization problem involving an \(n \times n\) kernel matrix, where \(n\) denotes the number of training samples, leading to quadratic growth in both computational time and memory usage \citep{dong2005}. This makes SVMs computationally expensive for large-scale data sets. Moreover, SVMs are inherently designed for binary classification, necessitating decomposition strategies such as One-vs-One and One-vs-All for multi-class problems \citep{hsu2002}. Their performance also tends to degrade in imbalanced data settings, where the decision boundary becomes biased toward the majority class \citep{cervantes2020}.

\subsection{Projection Pursuit Trees}\label{projection-pursuit-trees}

Projection pursuit methods \citep{friedman2006, kruskal1969} have been used to uncover low-dimensional projections that reveal structure in high-dimensional data; the \CRANpkg{PPtree} extends this idea into a recursive partitioning framework by optimizing a class-sensitive projection index at each node and then applying conventional split criteria on the resulting one-dimensional projection. This hybrid design allows the model to exploit multivariate correlations that axis-aligned trees often miss, while preserving the interpretability and hierarchical structure of tree models. In addition to improved separation in many settings, the PPtree produces node-specific projection coefficients that serve as transparent indicators of which variables drive each split, making it useful both for classification and for feature selection in multivariate problems.

\subsection{Hybrid Decision Trees with Support Vector Machines}\label{hybrid-decision-trees-with-support-vector-machines}

Support Vector Machine--based decision trees (DTSVMs) were first formalized by \citet{bennet1998}, who adapted Statistical Learning Theory and Structural Risk Minimization to construct binary decision trees in which each internal node is an SVM that partitions the data by an optimal hyperplane. This formulation enabled multivariate, margin-based decisions at every split. \citet{takahashi2002} extended the idea to multiclass settings by using a recursive partitioning strategy: the root node isolates the most separable class or classes from the remainder, and the procedure recurses until each leaf contains a single class, thereby covering the entire feature space.

\subsubsection{Scalability and optimization advances}\label{scalability-and-optimization-advances}

Subsequent studies have built upon this foundation to address scalability, optimization, and generalization issues. Optimal Decision Tree SVM (ODT-SVM) \citep{bala2011} introduced split-selection criteria based on Gini index, information gain, and scatter matrix separability to balance tree interpretability and margin-based precision.

\subsubsection{Oblique and Rotation-Based Extensions}\label{oblique-and-rotation-based-extensions}

Recent research generalizes DTSVMs through oblique splits and rotation-based ensemble strategies. Oblique Double Random Forests with MPSVM (MPDRaF) \citep{ganaie2022} implement multivariate (oblique) node splits using Multi-Plane SVM (MPSVM) formulations \citep{Mangasarian2006}, increasing the geometric flexibility of decision boundaries. These methods incorporate regularization variants (Tikhonov, axis-parallel, null-space) to mitigate small-sample issues at deeper nodes and improve generalization. Rotation-based Double Random Forests (DRaF) \citep{ganaie2022} apply PCA and LDA transforms at non-terminal nodes to generate diverse subspaces, enhancing ensemble diversity and stabilizing classification performance.

\subsubsection{Modern multi-class integration}\label{modern-multi-class-integration}

More recent approaches embed multiple SVM classifiers directly within each split to handle multi-class problems more efficiently. The Oblique Decision Tree Ensemble (ODTE) \citep{montanana2025} places one-vs-one or one-vs-rest SVMs at splits and dynamically selects the classifier that minimizes class impurity, enabling n-ary decisions within a single tree and addressing scalability and class-imbalance limitations inherent to binary SVM tree extensions.

\subsubsection{Synthesis and implications for future work}\label{synthesis-and-implications-for-future-work}

Across these developments, two recurring themes emerge: (1) leveraging multivariate, oblique decision boundaries to capture complex class geometry, and (2) combining transformation or ensemble strategies (rotations, multiple SVMs) to improve diversity and robustness. These trends point toward hybrid architectures that balance interpretability, margin-based generalization, and computational tractability---an agenda that motivates continued exploration of regularization schemes, split-selection heuristics, and efficient multi-class integration in SVM-based tree models.

\section{STree Alogrithm}\label{stree-alogrithm}

STree \citep{stree2021} is an oblique multi-class decision tree algorithm that integrates support vector machines (SVMs) to construct single margin‑based splits capable of handling multiclass problems. Unlike traditional multi-class tree methods that rely on clustering or ensembles of binary models, STree builds a single decision tree. Its central innovation lies in using SVM‑derived hyperplanes at each internal node, enabling more expressive splits than axis‑aligned trees while also being interpretable and computationally simple.

\subsection{Methodology}\label{methodology}

The algorithm generates a binary tree recursively. At each node:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Stopping conditions are evaluated to check the depth of the tree and the class purity. If the stopping conditions are met, a leaf node is created, labelled with the most frequent class in the node.
\item
  If the stopping conditions haven't been met the algorithm then selects the best hyperplane to split the data into two partitions: \(T^+\) (positive side) and \(T^-\) (negative side).
\end{enumerate}

When all instances at a node belong to more than two classes (\(k'>2\)), the approach generates multiple candidate binary splits: For each class label \(y_i\), it constructs a one-vs-rest binary problem: Class \(y_i\) vs.~all other classes. It trains an SVM for each of these \(k'\) problems, resulting in \(k'\) hyperplanes \(H_i\). Each hyperplane partitions the data into \(T_i^+\) and \(T_i^-\). The algorithm computes the impurity (using Shannon entropy) of the class distribution within each partition. When only two classes remain, STree trains a single SVM to obtain the maximum‑margin hyperplane. Instances are then routed left or right based on the sign of their distance to this hyperplane. Stree's performance depends heavily on hyperparameter tuning, including kernel choice (linear, polynomial or Gaussian), gamma, polynomial degree, regularization parameter C, and max iteration.

The algorithm was refined in future works by the author to include framework enabling

\begin{table}
\centering
\caption{\label{tab:stree-svmodt-benchmark-table}Comparing Mean Prediction Accuracy with default arguments - STree(R), STree(Python), SVMODT}
\centering
\begin{tabular}[t]{l|c|c|c}
\hline
Dataset & Stree(R) & STree(Python) & SVMODT\\
\hline
WDBC Diagnosis & \textbf{0.970 ± 0.004} & \textbf{0.970 ± 0.003} & \textbf{0.970 ± 0.004}\\
\hline
Iris & \textbf{0.966 ± 0.006} & 0.965 ± 0.010 & 0.965 ± 0.004\\
\hline
Echocardiogram & \textbf{0.846 ± 0.004} & 0.845 ± 0.008 & 0.845 ± 0.007\\
\hline
Fertility & 0.876 ± 0.011 & 0.874 ± 0.010 & \textbf{0.881 ± 0.000}\\
\hline
Wine & \textbf{0.978 ± 0.008} & 0.959 ± 0.008 & 0.975 ± 0.011\\
\hline
Cardiotography-3 & 0.901 ± 0.003 & \textbf{0.903 ± 0.003} & 0.898 ± 0.003\\
\hline
Cardiotography-10 & 0.763 ± 0.005 & \textbf{0.795 ± 0.018} & 0.760 ± 0.008\\
\hline
Ionosphere & \textbf{0.896 ± 0.009} & 0.892 ± 0.015 & \textbf{0.896 ± 0.007}\\
\hline
Dermatology & \textbf{0.972 ± 0.006} & 0.959 ± 0.004 & 0.967 ± 0.006\\
\hline
Statlog Australian Credit & \textbf{0.678 ± 0.000} & \textbf{0.678 ± 0.000} & \textbf{0.678 ± 0.000}\\
\hline
\end{tabular}
\end{table}

\section{Experiment}\label{experiment}

Table \ref{tab:stree-svmodt-benchmark-table}

\section{Acknowledgements}\label{acknowledgements}

The authors would like to thank Professor \textbf{Natalia Da Silva} and Professor \textbf{Dianne Helen Cook} for their constructive discussions and valuable insights regarding oblique decision trees and their implementations in R. The authors also acknowledges the use of OpenAI's GPT-5 model (ChatGPT) for editorial assistance and technical writing support during the preparation of this manuscript. All conceptual development, analysis, data processing, coding, and interpretation of results were performed independently by the author. The AI tool was used solely to refine language clarity, improve structure, and ensure stylistic consistency in the documentation.

\bibliography{RJreferences.bib}

\address{%
Aneesh Agarwal\\
Monash University\\%
\\
%
%
%
\href{mailto:aaga0022@student.monash.edu}{\nolinkurl{aaga0022@student.monash.edu}}%
}

\address{%
Jack Jewson\\
Monash University\\%
Department of Econometrics and Business Statistics, Monash University, Australia\\
%
%
%
\href{mailto:Jack.Jewson@monash.edu}{\nolinkurl{Jack.Jewson@monash.edu}}%
}

\address{%
Erik Sverdrup\\
Monash University\\%
Department of Econometrics and Business Statistics, Monash University, Australia\\
%
%
%
\href{mailto:Erik.Sverdrup@monash.edu}{\nolinkurl{Erik.Sverdrup@monash.edu}}%
}
