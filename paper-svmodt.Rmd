---
title: "svmodt: An R Package for Linear SVM-Based Oblique Decision Trees"
date: "2025-10-29"
abstract: >
  An abstract of less than 150 words.
draft: true
author:  
  - name: Aneesh Agarwal
    affiliation: Monash University
    email:  aaga0022@student.monash.edu
  - name: Jack Jewson
    affiliation:
    - Monash University
    address:
    - Department of Econometrics and Business Statistics, Monash University, Australia 
    email: Jack.Jewson@monash.edu
  - name: Erik Sverdrup
    affiliation:
    - Monash University
    address:
    - Department of Econometrics and Business Statistics, Monash University, Australia 
    email: Erik.Sverdrup@monash.edu
type: package
output: 
  rjtools::rjournal_article:
    self_contained: yes
    toc: no
header-includes:
  \usepackage{algorithm}
  \usepackage{algorithmic}
  \usepackage{amsmath}
  \usepackage{amssymb}
bibliography: RJreferences.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(kableExtra)
```

# Introduction

# Literature Review

## Decision Trees

Decision Trees (DTs) are interpretable classification models that represent their decision-making process through a hierarchical, tree-like structure. This structure comprises internal nodes containing splitting criteria and terminal (leaf) nodes corresponding to class labels. The nodes are connected by directed edges, each representing a possible outcome of a splitting criterion. Formally, a DT can be expressed as a rooted, directed tree $T = (G(V, E), v_1)$, where $V$ denotes the set of nodes, $E$ represents the set of edges linking these nodes, and $v_1$ is the root node.

If the tree $T$ has $m$ nodes, then for any $j \in \{1, \ldots, m\}$, the set of child nodes of $v_j \in V$ can be defined as:

$$
N^{+}(v_j) = \{ v_k \in V \mid k \in \{1, \ldots, m\},\; k \neq j,\; (v_j, v_k) \in E \}.
$$

Here, $N^{+}(v_j)$ denotes the set of nodes that are directly connected to $v_j$ through outgoing edges, representing all possible child nodes that can be reached from $v_j$ within the tree structure [@lopez2018].

Decision tree algorithms can be categorized based on whether the same type of test is applied at all internal nodes. **Homogeneous trees** employ a single algorithm throughout (e.g., univariate or multivariate splits), whereas **hybrid trees** allow different algorithms such as linear discriminant functions, $k$-nearest neighbors, or univariate splits that can be used in different subtrees [@brodley1995]. Hybrid trees exploit the principle of *selective superiority*, allowing subsets of the data to be modeled by the most appropriate classifier, thereby improving flexibility and accuracy.

### Univariate Decision Trees

Univariate Decision Trees (UDTs) trees represent axis-parallel hyperplanes dividing the instance space into several disjoint regions. Axis-parallel decision trees, such as CART and C4.5, represent two of the most widely used algorithms for classification tasks. The **CART (Classification and Regression Trees)** algorithm employs a binary recursive partitioning procedure capable of handling both continuous and categorical variables as predictors or targets. It operates directly on raw data without requiring binning. The tree is expanded recursively until no further splits are possible, after which **cost-complexity pruning** is applied to remove branches that contribute least to predictive performance. This pruning process generates a sequence of nested subtrees, from which the optimal model is selected using independent test data or cross-validation, rather than internal training measures [@breiman1984].

In contrast, **C4.5**, an extension of the earlier **ID3** algorithm [@quinlan1986], utilizing information theory measures such as **information gain** and **gain ratio** to select the most informative attribute for each split [@quinlan1993]. C4.5 also includes mechanisms to handle missing attribute values by weighting instances according to the proportion of known data and employs an **error-based pruning** method to reduce overfitting. Although these techniques are effective across diverse datasets, studies have shown that the choice of pruning strategy and stopping criteria can significantly affect model performance across different domains [@mingers1989; @schaffer1992].

While UDTs are highly interpretability, they are characterised by several representational limitations. Such trees often grow unnecessarily large, as they must approximate complex relationship between features through multiple axis-aligned partitions. This can result in the replication of subtrees and repeated testing of the same feature along different paths, both of which reduce efficiency and hinder generalization performance [@pagallo1990].

### Multivariate Decision Trees

Multivariate decision trees (MDTs) extends UDTs by allowing each internal node to perform splits based on linear or nonlinear combinations of multiple features. This flexibility enables the tree to form oblique decision boundaries that more accurately partition the instance space. For example, a single multivariate test such as $x + y < 8$ can replace multiple univariate splits needed to approximate the same boundary. The construction of MDTs introduces several design considerations, including how to represent multivariate tests, determine their coefficients, select features to include, handle symbolic and missing data, and prune to avoid overfitting [@brodley1995].

Various optimization algorithms—such as recursive least squares [@young1984], the pocket algorithm [@gallant1986], or thermal training [@frean1990]—may be used to estimate the weights. However, MDTs trade interpretability for representational power and often require additional mechanisms for **local feature selection**, such as *sequential forward selection* (SFS) or *sequential backward elimination* (SBE) [@kittler1986].

Empirical comparisons across multiple datasets demonstrate that multivariate trees generally achieve higher accuracy and smaller tree sizes than their univariate counterparts, though this comes at the cost of reduced interpretability. Moreover, MDTs retain key advantages of standard decision trees—such as sequential split criteria evaluation and transparent decision procedures—while offering improved modeling flexibility for complex datasets [@kozial2009; @friedl1997; @huan1998; @canete].

## Support Vector Machines (SVMs)

Support Vector Machines (SVMs) are powerful supervised learning models used for classification and regression tasks. They aim to determine an optimal separating hyperplane that maximizes the margin between different classes in the data. This margin-based approach enhances the generalization ability of the model, making SVMs robust and effective for many real-world problems [@cristianini2000].

### Linear SVMs

A simplest **linear SVMs** construct a separating hyperplane in an $n$-dimensional space such that the margin between the classes is maximized. Given a training dataset $\{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$, where $\mathbf{x}_i \in \mathbb{R}^n$ and $y_i \in \{-1, +1\}$, the decision function is defined as:

$$
f(\mathbf{x}) = \text{sign}(\mathbf{w}^\top \mathbf{x} + b),
$$

where $\mathbf{w}$ is the weight vector perpendicular to the hyperplane, and $b$ is the bias term. The optimal hyperplane is the one that maximizes the distance between the closest points of each class (the **support vectors**) and the hyperplane itself [@cortes1995].

Mathematically, the optimization problem for a hard-margin SVM (i.e., assuming the data are linearly separable) can be formulated as:

$$
\min_{\mathbf{w}, b} \ \frac{1}{2} \|\mathbf{w}\|^2
$$ subject to: $$
y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1, \quad i = 1, 2, \ldots, N.
$$

Here, $\|\mathbf{w}\|$ represents the norm of the weight vector and acts as a regularization term that controls the complexity of the model. The constraint ensures that all data points are correctly classified and lie outside the margin boundaries.

However, in most practical situations, perfect linear separability is not achievable. To address this, **soft-margin SVMs** introduce slack variables $\xi_i \geq 0$ to allow certain violations of the margin constraints, resulting in the following optimization problem:

$$
\min_{\mathbf{w}, b, \xi} \ \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{N} \xi_i
$$ subject to: $$
y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad i = 1, 2, \ldots, N.
$$

The parameter $C > 0$ controls the trade-off between maximizing the margin and minimizing the classification error on the training data. A large $C$ penalizes mis-classifications heavily, leading to a narrower margin, whereas a smaller $C$ allows more flexibility, potentially improving generalization in the presence of noise.

The solution to this constrained optimization problem is obtained using **Lagrange multipliers**, resulting in a dual formulation expressed as:

$$
\max_{\alpha} \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j \langle \mathbf{x}_i, \mathbf{x}_j \rangle
$$ subject to: $$
\sum_{i=1}^{N} \alpha_i y_i = 0, \quad 0 \leq \alpha_i \leq C.
$$

The data points corresponding to non-zero $\alpha_i$ values are the **support vectors**, which define the decision boundary. The resulting decision function for a new observation $\mathbf{x}$ is given by:

$$
f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^{N} \alpha_i y_i \langle \mathbf{x}_i, \mathbf{x} \rangle + b\right).
$$

This formulation highlights one of the most important properties of SVMs — the decision boundary depends only on a subset of the training data (the support vectors), making SVMs both efficient and robust in representing the learned model [@cervantes2020].

### Non-Linear SVMs

While linear classifiers provide useful insights, they are often inadequate for real-world datasets, where classes are not linearly separable. In such cases, SVMs can be extended to create **nonlinear decision boundaries** by mapping the input vectors into a higher-dimensional **feature space** using a nonlinear transformation $\phi: \mathbb{R}^n \rightarrow \mathcal{F}$. The linear transformation is then achieved in the transformed space using:

$$
f(\mathbf{x}) = \text{sign}(\mathbf{w}^\top \phi(\mathbf{x}) + b).
$$

However, directly computing $\phi(\mathbf{x})$ can be computationally expensive. To address this, SVMs employ the **kernel trick**, where the dot product in the feature space is replaced with a kernel function $K(\mathbf{x}_i, \mathbf{x}_j)$:

$$
K(\mathbf{x}_i, \mathbf{x}_j) = \langle \phi(\mathbf{x}_i), \phi(\mathbf{x}_j) \rangle.
$$

The resulting decision function becomes:

$$
f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^{N} \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b\right),
$$

where $\alpha_i$ are the Lagrange multipliers obtained during training.

For a function $K$ to be a valid kernel, it must satisfy **Mercer’s condition** [@vapnik2013] i.e., the kernel matrix must be symmetric and positive semi-definite. Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:kernels-tab-interactive)', '\\@ref(tab:kernels-tab-static)'))`

```{r kernels-tab-interactive, eval = knitr::is_html_output()}
kernel_table <- data.frame(
  "Kernel Type" = c("Linear", "Polynomial", "Gaussian", "RBF", "Sigmoid"),
  "Mathematical Definition" = c(
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^\\top \\mathbf{x}_j$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^\\top \\mathbf{x}_j + 1)^d$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\kappa \\mathbf{x}_i^\\top \\mathbf{x}_j + \\theta)$"
  ),
  "Key Parameters" = c(
    "None",
    "Degree $d$",
    "Bandwidth $\\sigma$",
    "$\\gamma$",
    "$\\kappa, \\theta$"
  ),
  check.names = FALSE
)

kable(
  kernel_table,
  escape = FALSE,
  format = "html",
  caption = "Commonly used kernel functions and their parameters."
) |> kable_styling(full_width = FALSE, position = "center")
```

```{r kernels-tab-static, eval = knitr::is_latex_output(), }
kernel_table <- data.frame(
  "Kernel Type" = c("Linear", "Polynomial", "Gaussian", "RBF", "Sigmoid"),
  "Mathematical Definition" = c(
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^\\top \\mathbf{x}_j$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^\\top \\mathbf{x}_j + 1)^d$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\kappa \\mathbf{x}_i^\\top \\mathbf{x}_j + \\theta)$"
  ),
  "Key Parameters" = c(
    "None",
    "Degree $d$",
    "Bandwidth $\\sigma$",
    "$\\gamma$",
    "$\\kappa, \\theta$"
  ),
  check.names = FALSE
)

kable(
  kernel_table,
  escape = FALSE,
  format = "latex",
  caption = "Commonly used kernel functions and their parameters."
) |>
  kable_styling(font_size = 7, full_width = FALSE, position = "center")
```

Although SVMs exhibit strong theoretical foundations and robust generalization capabilities, they present several practical limitations. Model performance is highly dependent on the appropriate selection of hyperparameters such as the regularization term ($C$) and kernel parameters (e.g., $\gamma$), which govern the trade-off between margin maximization and misclassification tolerance [@nanda2018]. Training an SVM requires solving a quadratic programming (QP) optimization problem involving an $n \times n$ kernel matrix, where $n$ denotes the number of training samples, leading to quadratic growth in both computational time and memory usage [@dong2005]. This makes SVMs computationally expensive for large-scale datasets. Moreover, SVMs are inherently designed for binary classification, necessitating decomposition strategies such as One-vs-One and One-vs-All for multi-class problems [@hsu2002]. Their performance also tends to degrade in imbalanced data settings, where the decision boundary becomes biased toward the majority class [@cervantes2020].

## Hybrid Decision Trees with Support Vector Machines

The earliest formalization of Support Vector Machine–based Decision Trees (DTSVMs) was introduced by @bennet1998 , who extended the principles of Statistical Learning Theory [@vapnik2013] and Structural Risk Minimization (SRM) to the construction of binary decision trees. Each node of a decision tree was treated as an SVM that partitions data along an optimal hyperplane. This approach allowed for multivariate SVM decisions at each node. @takahashi2002 extended DTSVMs to multiclass problems, using a recursive tree-based partitioning where the root separates the most separable class (or classes) from the rest. Subsequent nodes repeat this process until each leaf contains a single class, ensuring that all regions of the feature space are classified.

Subsequent studies have built upon this foundation to address scalability, optimization, and generalization issues. Optimal Decision Tree SVM (ODT-SVM) [@bala2011] introduced split-selection criteria based on Gini index, information gain, and scatter matrix separability to balance tree interpretability and margin-based precision.

### Oblique and Rotation-Based Extensions

Recent research has explored oblique and rotation-based decision ensembles that generalize the DTSVM concept. Oblique Double Random Forests with MPSVM (MPDRaF) [@ganaie2022] introduce multivariate (oblique) splits at each node using Multi-Plane SVM (MPSVM) formulations [@Mangasarian2006], enhancing the geometric flexibility of decision boundaries. Regularization variants—such as Tikhonov, axis-parallel, and null-space—address sample-size limitations at deeper nodes, ensuring better generalization.

Similarly, Rotation-based Double Random Forests (DRaF) [@ganaie2022] employ Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) transformations at non-terminal nodes to create diverse subspaces, improving ensemble diversity and classification stability.

### Modern Multi-class Integration

Recent works such as the Oblique Decision Tree Ensemble (ODTE) [@montanana2025] embed multiple SVM classifiers (e.g., one-vs-one or one-vs-rest) directly at each split, dynamically selecting the model that minimizes class impurity. This enables efficient n-ary classification within a single tree structure, mitigating the scalability and imbalance limitations of traditional binary SVM extensions.

# Algorithm Description

## Core Methodology

The SVM Oblique Decision Tree (SVMODT) constructs a binary decision tree where each internal node $v$ at depth $d$. The algorithm is implemneted in R using the \CRANpkg{e1071} performs the following operations:

1.  Feature Selection: Dynamically selects $m_d$ features from the available feature set $\mathcal{F}$ using one of three strategies: Random, sampling, Mutual information ranking (via \CRANpkg{FSelectorRcpp}), Correlation-based selection

2.  Feature Scaling: Applies z-score normalization to selected features $\mathbf{X}_v \in \mathbb{R}^{n \times m_d}$ $$\mathbf{X}_v^{\text{scaled}} = \frac{\mathbf{X}_v - \boldsymbol{\mu}_v}{\boldsymbol{\sigma}_v}$$

3.  SVM Training: Fits a linear SVM with optional class weights $w_c$ to handle imbalanced data:

$$\min_{\mathbf{w}, b} \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^{n} w_{y_i}\xi_i$$

4.  Node Splitting: Computes decision values $f(x_i) = w^Tx_i+b$ and partitions samples:

    -   Left child: ${i:f(x_i)>0}$

    -   Right child: ${i:f(x_i)\leq0}$

```{=latex}
\begin{algorithm}
\caption{SVM Oblique Decision Tree Construction}
\label{alg:svmodt}
\begin{algorithmic}[1]
\STATE \textbf{Procedure} SVMSplit($\mathcal{D}, d, d_{\max}, n_{\min}$)
    \STATE $n \gets |\mathcal{D}|$, $\mathcal{Y} \gets \text{labels}(\mathcal{D})$
    
    \IF{$d > d_{\max}$ \textbf{or} $|\text{unique}(\mathcal{Y})| = 1$ \textbf{or} $n < n_{\min}$}
        \RETURN $\text{LeafNode}(\mathcal{Y})$ \COMMENT{Stopping criteria}
    \ENDIF
    
    \STATE $m_d \gets \text{DynamicMaxFeatures}(d, m_{\text{base}}, \text{strategy})$
    \STATE $\mathcal{F}_d \gets \text{SelectFeatures}(\mathcal{D}, m_d, \text{method})$
    \STATE $\mathbf{X}_{\text{scaled}}, \text{scaler} \gets \text{Scale}(\mathcal{D}[\mathcal{F}_d])$
    
    \IF{$|\mathcal{F}_d| = 0$}
        \RETURN $\text{LeafNode}(\mathcal{Y})$ \COMMENT{No valid features}
    \ENDIF
    
    \STATE $w_c \gets \text{CalculateClassWeights}(\mathcal{Y}, \text{strategy})$
    \STATE $\text{SVM} \gets \text{FitLinearSVM}(\mathbf{X}_{\text{scaled}}, \mathcal{Y}, w_c)$
    
    \IF{$\text{SVM} = \text{null}$}
        \RETURN $\text{LeafNode}(\mathcal{Y})$ \COMMENT{SVM fitting failed}
    \ENDIF
    
    \STATE $\mathbf{f} \gets \text{SVM.DecisionValues}(\mathbf{X}_{\text{scaled}})$
    \STATE $\mathcal{I}_L \gets \{i : f_i > 0\}$, $\mathcal{I}_R \gets \{i : f_i \leq 0\}$
    
    \IF{$|\mathcal{I}_L| = 0$ \textbf{or} $|\mathcal{I}_R| = 0$}
        \RETURN $\text{LeafNode}(\mathcal{Y})$ \COMMENT{Ineffective split}
    \ENDIF
    
    \STATE $\text{left} \gets \textsc{SVMSplit}(\mathcal{D}[\mathcal{I}_L], d+1, d_{\max}, n_{\min})$
    \STATE $\text{right} \gets \textsc{SVMSplit}(\mathcal{D}[\mathcal{I}_R], d+1, d_{\max}, n_{\min})$
    
    \RETURN $\text{InternalNode}(\text{SVM}, \mathcal{F}_d, \text{scaler}, \text{left}, \text{right})$
\STATE \textbf{End Procedure}
\end{algorithmic}
\end{algorithm}
```

## Key Features

The algorithm supports three feature selection strategies with optional penalization of previously used features:

1.  Maximum features at depth $d$: $$
    m_d =
    \begin{cases}
    m_{\text{base}} & \text{constant strategy} \\
    \lfloor m_{\text{base}} \cdot \alpha^{d-1} \rfloor & \text{decrease strategy} \\
    \text{Uniform}(\lfloor p \cdot \ell_{\min} \rfloor, \lfloor p \cdot \ell_{\max} \rfloor) & \text{random strategy}
    \end{cases}
    $$where $(\alpha \in (0, 1])$ is the decrease rate, $(p)$ is the total number of features, $(\ell_{\min})$ and $(\ell_{\max})$ define the bounds (as fractions of $(p)$) for the random strategy, and $(m_{\text{base}})$ is the base number of features at depth 1.

<!-- -->

2.  Feature penalty: Previously used features have their selection weight reduced by factor $(1 - \lambda)$ where $\lambda \in [0, 1)$.

3.  Class Weight Handling To address class imbalance, the algorithm supports four weighting schemes:

    -   None: $w_c = 1$ for all classes,

    -   Balanced: $cw_c = \frac{n}{K \cdot n_c}$ where $K$ is the number of classes and $n_c$ is the sample count for class $c$

    -   Balanced sub-sample: $w_c = \frac{1}{n_c} \cdot \frac{K}{\sum_{c'} 1/n_{c'}}$

    -   Custom: User-defined weights.

All weights are capped at 10 to prevent numerical instability.
