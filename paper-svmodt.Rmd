---
title: "svmodt: An R Package for Linear SVM-Based Oblique Decision Trees"
date: "2025-10-29"
abstract: >
  An abstract of less than 150 words.
draft: true
author:  
  - name: Aneesh Agarwal
    affiliation: Monash University
    email:  aaga0022@student.monash.edu
  - name: Jack Jewson
    affiliation:
    - Monash University
    address:
    - Department of Econometrics and Business Statistics, Monash University, Australia 
    email: Jack.Jewson@monash.edu
  - name: Erik Sverdrup
    affiliation:
    - Monash University
    address:
    - Department of Econometrics and Business Statistics, Monash University, Australia 
    email: Erik.Sverdrup@monash.edu
type: package
output: 
  rjtools::rjournal_article:
    self_contained: yes
    toc: no
header-includes:
  \usepackage{algorithm}
  \usepackage{algorithmic}
  \usepackage{amsmath}
  \usepackage{amssymb}
  \providecommand{\pandocbounded}[1]{#1}
bibliography: RJreferences.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
knitr::opts_knit$set(latex_engine = "xelatex")
library(kableExtra)
library(dplyr)
#devtools::install_github("AneeshAgarwala/svmodt")
library(svmodt)
library(palmerpenguins)
library(rpart)
library(kernlab)
library(rsample)
library(ggplot2)
library(patchwork)
```

# Introduction

# Background

## Decision Trees

Decision Trees (DTs) are interpretable classification models that represent their decision-making process through a hierarchical, tree-like structure. This structure comprises internal nodes containing splitting criteria and terminal (leaf) nodes corresponding to class labels. The nodes are connected by directed edges, each representing a possible outcome of a splitting criterion. Formally, a DT can be expressed as a rooted, directed tree $T = (G(V, E), v_1)$, where $V$ denotes the set of nodes, $E$ represents the set of edges linking these nodes, and $v_1$ is the root node.

If the tree $T$ has $m$ nodes, then for any $j \in \{1, \ldots, m\}$, the set of child nodes of $v_j \in V$ can be defined as:

$$
N^{+}(v_j) = \{ v_k \in V \mid k \in \{1, \ldots, m\},\; k \neq j,\; (v_j, v_k) \in E \}.
$$

Here, $N^{+}(v_j)$ denotes the set of nodes that are directly connected to $v_j$ through outgoing edges, representing all possible child nodes that can be reached from $v_j$ within the tree structure [@lopez2018].

Decision tree algorithms can be categorized based on whether the same type of test is applied at all internal nodes. **Homogeneous trees** employ a single algorithm throughout (e.g., univariate or multivariate splits), whereas **hybrid trees** allow different algorithms such as linear discriminant functions, $k$-nearest neighbors, or univariate splits that can be used in different subtrees [@brodley1995]. Hybrid trees exploit the principle of *selective superiority*, allowing subsets of the data to be modeled by the most appropriate classifier, thereby improving flexibility and accuracy.

### Univariate Decision Trees

Univariate Decision Trees (UDTs) trees represent axis-parallel hyperplanes dividing the instance space into several disjoint regions. Axis-parallel decision trees, such as CART and C4.5, represent two of the most widely used algorithms for classification tasks. The **CART (Classification and Regression Trees)** algorithm employs a binary recursive partitioning procedure capable of handling both continuous and categorical variables as predictors or targets. It operates directly on raw data without requiring binning. The tree is expanded recursively until no further splits are possible, after which **cost-complexity pruning** is applied to remove branches that contribute least to predictive performance. This pruning process generates a sequence of nested subtrees, from which the optimal model is selected using independent test data or cross-validation, rather than internal training measures [@breiman1984].

In contrast, **C4.5**, an extension of the earlier **ID3** algorithm [@quinlan1986], utilizing information theory measures such as **information gain** and **gain ratio** to select the most informative attribute for each split [@quinlan1993]. C4.5 also includes mechanisms to handle missing attribute values by weighting instances according to the proportion of known data and employs an **error-based pruning** method to reduce overfitting. Although these techniques are effective across diverse datasets, studies have shown that the choice of pruning strategy and stopping criteria can significantly affect model performance across different domains [@mingers1989; @schaffer1992].

While UDTs are highly interpretability, they are characterised by several representational limitations. Such trees often grow unnecessarily large, as they must approximate complex relationship between features through multiple axis-aligned partitions. This can result in the replication of subtrees and repeated testing of the same feature along different paths, both of which reduce efficiency and hinder generalization performance [@pagallo1990].

### Multivariate Decision Trees

Multivariate decision trees (MDTs) extends UDTs by allowing each internal node to perform splits based on linear or nonlinear combinations of multiple features. This flexibility enables the tree to form oblique decision boundaries that more accurately partition the instance space. For example, a single multivariate test such as $x + y < 8$ can replace multiple univariate splits needed to approximate the same boundary. The construction of MDTs introduces several design considerations, including how to represent multivariate tests, determine their coefficients, select features to include, handle symbolic and missing data, and prune to avoid overfitting [@brodley1995].

Various optimization algorithms—such as recursive least squares [@young1984], the pocket algorithm [@gallant1986], or thermal training [@frean1990]—may be used to estimate the weights. However, MDTs trade interpretability for representational power and often require additional mechanisms for **local feature selection**, such as *sequential forward selection* (SFS) or *sequential backward elimination* (SBE) [@kittler1986].

Empirical comparisons across multiple datasets demonstrate that multivariate trees generally achieve higher accuracy and smaller tree sizes than their univariate counterparts, though this comes at the cost of reduced interpretability. Moreover, MDTs retain key advantages of standard decision trees—such as sequential split criteria evaluation and transparent decision procedures—while offering improved modeling flexibility for complex datasets [@kozial2009; @friedl1997; @huan1998; @canete].

## Support Vector Machines (SVMs)

Support Vector Machines (SVMs) are powerful supervised learning models used for classification and regression tasks. They aim to determine an optimal separating hyperplane that maximizes the margin between different classes in the data. This margin-based approach enhances the generalization ability of the model, making SVMs robust and effective for many real-world problems [@cristianini2000].

### Linear SVMs

A simplest **linear SVMs** construct a separating hyperplane in an $n$-dimensional space such that the margin between the classes is maximized. Given a training dataset $\{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$, where $\mathbf{x}_i \in \mathbb{R}^n$ and $y_i \in \{-1, +1\}$, the decision function is defined as:

$$
f(\mathbf{x}) = \text{sign}(\mathbf{w}^\top \mathbf{x} + b),
$$

where $\mathbf{w}$ is the weight vector perpendicular to the hyperplane, and $b$ is the bias term. The optimal hyperplane is the one that maximizes the distance between the closest points of each class (the **support vectors**) and the hyperplane itself [@cortes1995].

Mathematically, the optimization problem for a hard-margin SVM (i.e., assuming the data are linearly separable) can be formulated as:

$$
\min_{\mathbf{w}, b} \ \frac{1}{2} \|\mathbf{w}\|^2
$$ subject to: $$
y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1, \quad i = 1, 2, \ldots, N.
$$

Here, $\|\mathbf{w}\|$ represents the norm of the weight vector and acts as a regularization term that controls the complexity of the model. The constraint ensures that all data points are correctly classified and lie outside the margin boundaries.

However, in most practical situations, perfect linear separability is not achievable. To address this, **soft-margin SVMs** introduce slack variables $\xi_i \geq 0$ to allow certain violations of the margin constraints, resulting in the following optimization problem:

$$
\min_{\mathbf{w}, b, \xi} \ \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{N} \xi_i
$$ subject to: $$
y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad i = 1, 2, \ldots, N.
$$

The parameter $C > 0$ controls the trade-off between maximizing the margin and minimizing the classification error on the training data. A large $C$ penalizes mis-classifications heavily, leading to a narrower margin, whereas a smaller $C$ allows more flexibility, potentially improving generalization in the presence of noise.

The solution to this constrained optimization problem is obtained using **Lagrange multipliers**, resulting in a dual formulation expressed as:

$$
\max_{\alpha} \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j \langle \mathbf{x}_i, \mathbf{x}_j \rangle
$$ subject to: $$
\sum_{i=1}^{N} \alpha_i y_i = 0, \quad 0 \leq \alpha_i \leq C.
$$

The data points corresponding to non-zero $\alpha_i$ values are the **support vectors**, which define the decision boundary. The resulting decision function for a new observation $\mathbf{x}$ is given by:

$$
f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^{N} \alpha_i y_i \langle \mathbf{x}_i, \mathbf{x} \rangle + b\right).
$$

This formulation highlights one of the most important properties of SVMs — the decision boundary depends only on a subset of the training data (the support vectors), making SVMs both efficient and robust in representing the learned model [@cervantes2020].

### Non-Linear SVMs

While linear classifiers provide useful insights, they are often inadequate for real-world datasets, where classes are not linearly separable. In such cases, SVMs can be extended to create **nonlinear decision boundaries** by mapping the input vectors into a higher-dimensional **feature space** using a nonlinear transformation $\phi: \mathbb{R}^n \rightarrow \mathcal{F}$. The linear transformation is then achieved in the transformed space using:

$$
f(\mathbf{x}) = \text{sign}(\mathbf{w}^\top \phi(\mathbf{x}) + b).
$$

However, directly computing $\phi(\mathbf{x})$ can be computationally expensive. To address this, SVMs employ the **kernel trick**, where the dot product in the feature space is replaced with a kernel function $K(\mathbf{x}_i, \mathbf{x}_j)$:

$$
K(\mathbf{x}_i, \mathbf{x}_j) = \langle \phi(\mathbf{x}_i), \phi(\mathbf{x}_j) \rangle.
$$

The resulting decision function becomes:

$$
f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^{N} \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b\right),
$$

where $\alpha_i$ are the Lagrange multipliers obtained during training.

For a function $K$ to be a valid kernel, it must satisfy **Mercer’s condition** [@vapnik2013] i.e., the kernel matrix must be symmetric and positive semi-definite. Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:kernels-tab-interactive)', '\\@ref(tab:kernels-tab-static)'))`

```{r kernels-tab-interactive, eval = knitr::is_html_output()}
kernel_table <- data.frame(
  "Kernel Type" = c("Linear", "Polynomial", "Gaussian", "RBF", "Sigmoid"),
  "Mathematical Definition" = c(
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^\\top \\mathbf{x}_j$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^\\top \\mathbf{x}_j + 1)^d$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\kappa \\mathbf{x}_i^\\top \\mathbf{x}_j + \\theta)$"
  ),
  "Key Parameters" = c(
    "None",
    "Degree $d$",
    "Bandwidth $\\sigma$",
    "$\\gamma$",
    "$\\kappa, \\theta$"
  ),
  check.names = FALSE
)

kable(
  kernel_table,
  escape = FALSE,
  format = "html",
  caption = "Commonly used kernel functions and their parameters."
) |> kable_styling(full_width = FALSE, position = "center")
```

```{r kernels-tab-static, eval = knitr::is_latex_output()}
kernel_table <- data.frame(
  "Kernel Type" = c("Linear", "Polynomial", "Gaussian", "RBF", "Sigmoid"),
  "Mathematical Definition" = c(
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^\\top \\mathbf{x}_j$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^\\top \\mathbf{x}_j + 1)^d$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)$",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\kappa \\mathbf{x}_i^\\top \\mathbf{x}_j + \\theta)$"
  ),
  "Key Parameters" = c(
    "None",
    "Degree $d$",
    "Bandwidth $\\sigma$",
    "$\\gamma$",
    "$\\kappa, \\theta$"
  ),
  check.names = FALSE
)

kable(
  kernel_table,
  escape = FALSE,
  format = "latex",
  caption = "Commonly used kernel functions and their parameters."
) |>
  kable_styling(font_size = 7, full_width = FALSE, position = "center")
```

Although SVMs exhibit strong theoretical foundations and robust generalization capabilities, they present several practical limitations. Model performance is highly dependent on the appropriate selection of hyperparameters such as the regularization term ($C$) and kernel parameters (e.g., $\gamma$), which govern the trade-off between margin maximization and misclassification tolerance [@nanda2018]. Training an SVM requires solving a quadratic programming (QP) optimization problem involving an $n \times n$ kernel matrix, where $n$ denotes the number of training samples, leading to quadratic growth in both computational time and memory usage [@dong2005]. This makes SVMs computationally expensive for large-scale datasets. Moreover, SVMs are inherently designed for binary classification, necessitating decomposition strategies such as One-vs-One and One-vs-All for multi-class problems [@hsu2002]. Their performance also tends to degrade in imbalanced data settings, where the decision boundary becomes biased toward the majority class [@cervantes2020].

## Hybrid Decision Trees with Support Vector Machines

The earliest formalization of Support Vector Machine–based Decision Trees (DTSVMs) was introduced by @bennet1998 , who extended the principles of Statistical Learning Theory [@vapnik2013] and Structural Risk Minimization (SRM) to the construction of binary decision trees. Each node of a decision tree was treated as an SVM that partitions data along an optimal hyperplane. This approach allowed for multivariate SVM decisions at each node. @takahashi2002 extended DTSVMs to multiclass problems, using a recursive tree-based partitioning where the root separates the most separable class (or classes) from the rest. Subsequent nodes repeat this process until each leaf contains a single class, ensuring that all regions of the feature space are classified.

Subsequent studies have built upon this foundation to address scalability, optimization, and generalization issues. Optimal Decision Tree SVM (ODT-SVM) [@bala2011] introduced split-selection criteria based on Gini index, information gain, and scatter matrix separability to balance tree interpretability and margin-based precision.

### Oblique and Rotation-Based Extensions

Recent research has explored oblique and rotation-based decision ensembles that generalize the DTSVM concept. Oblique Double Random Forests with MPSVM (MPDRaF) [@ganaie2022] introduce multivariate (oblique) splits at each node using Multi-Plane SVM (MPSVM) formulations [@Mangasarian2006], enhancing the geometric flexibility of decision boundaries. Regularization variants—such as Tikhonov, axis-parallel, and null-space—address sample-size limitations at deeper nodes, ensuring better generalization.

Similarly, Rotation-based Double Random Forests (DRaF) [@ganaie2022] employ Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) transformations at non-terminal nodes to create diverse subspaces, improving ensemble diversity and classification stability.

### Modern Multi-class Integration

Recent works such as the Oblique Decision Tree Ensemble (ODTE) [@montanana2025] embed multiple SVM classifiers (e.g., one-vs-one or one-vs-rest) directly at each split, dynamically selecting the model that minimizes class impurity. This enables efficient n-ary classification within a single tree structure, mitigating the scalability and imbalance limitations of traditional binary SVM extensions.

## Related Software Packages

Several software packages provide functionality related to SVM-based decision trees, but fully-featured implementations of SVM Oblique Decision Trees (SVMODT) remain limited, particularly in R. In R, very scarce SVMODT implementation is available. The \CRANpkg{PPforest} package implements projection pursuit random forests, where trees are constructed by splitting on linear combinations of variables to maximize class separation, supporting multi-class problems and variable importance measures. In Python, **STree** [@Montanana2021] provides an oblique decision tree classifier based on SVM nodes, where each node is built and split using scikit-learn’s **SVC** models [@scikit2011]. Java-based frameworks such as **Weka** provide classical decision trees (**J48**) and SVMs (**SMO**) that can be adapted for hierarchical SVM trees [@menkovski2008]. Overall, while multiple languages provide the building blocks for SVM training, decision tree construction, and ensemble methods—there is currently no widely available, fully integrated SVMODT software in R, highlighting the need for custom implementations or adaptation of existing tools for research and applied purposes.

# Research Gaps & Contributions

## Identified Research Gaps

Despite significant advances in decision tree methodologies and SVM-based hybrid approaches, several critical limitations persist in existing implementations:

### Limited Flexibility in Oblique Decision Tree Software

As discussed before, while oblique decision trees using SVMs have been theoretically established since @bennet1998, accessible and feature-rich implementations remain scarce, particularly in R. Existing approaches such as Optimal Decision Tree SVM (ODT-SVM) [@bala2011] and recent extensions like MPDRaF [@ganaie2022] and ODTE [@montanana2025] have advanced the field theoretically, but face several practical limitations:

-   **Software accessibility**: The \CRANpkg{PPforest} package provides projection pursuit forests but lacks true SVM-based splitting. Python's **STree** [@Montanana2021] offers oblique trees but is not available in R, leaving a significant gap in the R ecosystem for practitioners who prefer or require R-based workflows.

-   **Limited customization**: Existing implementations typically offer rigid feature selection mechanisms, with most using either all features or a fixed random subset at each split. There is very limited support for depth-dependent feature selection strategies or mechanisms to encourage feature diversity across tree paths.

-   **Class imbalance handling**: While SVMs inherently support class weighting, most hybrid DTSVM implementations apply weights globally at the tree level rather than computing node-specific weights that adapt to local class distributions encountered during recursive splitting.

-   **Interpret-ability tools**: Despite the advantage of tree-based models in interpret ability, existing packages lack comprehensive visualization tools for examining individual SVM hyper-planes, tracing prediction paths, or understanding how oblique splits contribute to final decisions.

### Absence of Integrated Pruning in Modern Decision Trees

Classical decision tree algorithms, such as CART [@breiman1984], typically employ a two-phase process: a maximal tree is first grown, and pruning is applied afterward using cost-complexity analysis with cross-validation. This approach introduces several inefficiencies. Branches that will ultimately be pruned are fully constructed, consuming unnecessary computational resources, and practitioners must tune both tree-growing parameters (e.g., depth, minimum samples) and pruning parameters ($\alpha$), increasing model selection complexity. Furthermore, deferring pruning to a post-processing phase prevents early stopping opportunities that could reduce training time. While C4.5 [@quinlan1993] implemented error-based pruning and some modern algorithms include early stopping criteria, integrated cost-complexity pruning; where pruning decisions are made in real time during tree construction remains largely unexplored, despite its potential to improve computational efficiency.

## Our Contributions

This paper introduces the **svmodt** R package, which addresses the limitations identified in existing decision tree and SVM-based methods through two complementary algorithms and a comprehensive software implementation. Our contributions span algorithmic innovation, theoretical analysis, and practical software engineering.

1.  **SVM-Based Oblique Decision Trees with Advanced Features**\
    We propose a flexible oblique decision tree algorithm that uses linear SVMs at each internal node. The implementation offers several novel features:
    1.  **Dynamic Feature Selection Strategies:** We introduce three strategies to control the number of features considered at each depth $d$:
        -   *Constant strategy:* fixed number of features at all depths;
        -   *Decrease strategy:* the feature count decreases with depth, mitigating over-fitting and improving computational efficiency;
        -   *Random strategy:* a random subset of features is selected at each node, promoting diversity in feature usage. These strategies extend beyond the fixed $m = \sqrt{p}$ approach used in Random Forests, giving practitioners explicit control over feature complexity throughout the tree.
    2.  **Feature Diversity via Penalization:** To reduce redundancy and encourage diverse feature usage, features used in ancestor nodes have their selection weights reduced by a factor $1-\lambda$ controlled via the `penalize_used_features` and `feature_penalty_weight` parameters. Unlike naive random sampling, this approach explicitly discourages repeated feature use while maintaining flexibility.
    3.  **Node-Specific Class Weighting:** Four weighting strategies are implemented, with weights recalculated at each node based on local class distributions: none, balanced, balanced sub-sample, and custom user-defined weights. All weights are capped to prevent numerical instability, enabling adaptive handling of class imbalance throughout the tree.

<!-- -->

2.  **Decision Tree with Integrated Cost-Complexity Pruning**\
    We implement a decision tree algorithm where pruning occurs during construction rather than as a post-processing step. At each node, the algorithm immediately compares the cost-complexity of a leaf versus its sub-tree and prunes in real time if beneficial. This approach eliminates the need for separate pruning phases, reduces computational cost, simplifies hyper-parameter tuning, and maintains optimal equivalent to CART’s post-pruning.

3.  **Comprehensive R Package Implementation**\
    The **svmodt** package provides a production-ready implementation with a well-documented API that includes type checking and input validation to ensure robust and user-friendly operation. It supports both binary and multi-class classification and leverages vectorized operations for computational efficiency. For interpret-ability and diagnostics, the package includes functions such as `print_svm_tree()` for hierarchical ASCII visualization, `trace_prediction_path()` for sample-level decision path tracing, and `visualize_svm_tree()` for automated 2D plotting of SVM hyper-planes. The package also comes with included data sets, such as the Wisconsin Diagnostic Breast Cancer (WDBC) data set. Comprehensive documentation is provided, including vignettes and Roxygen2-based function documentation, along with reproducible example code to facilitate practical application and comparison with alternative methods.

# Methodology

To better understand the structure and mechanics of decision trees, we first implemented a standard axis-parallel decision tree in R from scratch before extending it into the oblique (SVM-based) variant.

## Custom Univariate Decision Tree with Integrated Cost-Complexity Pruning

This implementation presents a decision tree algorithm with **integrated cost-complexity pruning**, where pruning decisions are made during tree construction rather than as a post-processing step. The algorithm supports multiple splitting criteria and performs real-time pruning based on cost-complexity analysis.

### Algorithm Description

At each node $v$ with samples $\mathcal{D}_v$ and labels $\mathcal{Y}_v$, the algorithm:

1.  **Compute Node Statistics**: At each node of the decision tree, several key statistics are computed to guide the splitting process. The class probabilities are calculated as $p_c = \frac{n_c}{n},$ where $n_c$ is the count of samples belonging to class $c$ in the node, and $n = |D_v|$ is the total number of samples at the node. The predicted class is assigned as the class with the highest probability, $$\hat{y} = \arg\max_c p_c.$$

2.  **Select Optimal Split**: The optimal split at each node is determined using one of three criteria:

    -   **Gini Impurity** is defined as $$\text{Gini}(\mathcal{Y}) = 1 - \sum_{c=1}^{K} p_c^2,$$where $p_c$ is the proportion of samples belonging to class $c$ in node $\mathcal{Y}$ [@breiman1984].

    -   **Information Gain** evaluates the reduction in node impurity and is computed as $$\text{IG}(f, s) = H(\mathcal{Y}_v) - \sum_{i \in \{L, R\}} \frac{n_i}{n} H(\mathcal{Y}_i),$$where $H(\mathcal{Y})$ denotes the impurity of a node (entropy or Gini), $n_i$ is the number of samples in the child node, and $n$ is the number of samples in the parent node [@breiman1984[.

    -   **Gain Ratio** further adjusts Information Gain by penalizing splits that create many small partitions [@quinlan1993] and is defined as $$\text{GR}(f, s) = \frac{\text{IG}(f, s)}{\text{SI}(f, s)},$$where the split information is $$\text{SI}(f, s) = -\sum_{i \in \{L, R\}} \frac{n_i}{n} \log_2\left(\frac{n_i}{n}\right).$$

3.  **Evaluates Pruning Criteria**: To ensure the tree remains interpret-able and avoids over-fitting, integrated pruning is performed using cost-complexity analysis. The cost of a sub-tree $T_v$ is given by $$R_\alpha(T_v) = R(T_v) + \alpha \|T_v\|,$$ where $R(T_v)$ is the miss-classification error, $|T_v|$ is the number of leaves in the sub-tree, and $\alpha \geq 0$ is a complexity parameter controlling the trade-off between accuracy and simplicity. For a single leaf node $t_v$, the cost is $$R_\alpha(t_v) = R(t_v) + \alpha.$$ A sub-tree $T_v$ is pruned, i.e., replaced by a single leaf node, if $$R_\alpha(t_v) \leq R_\alpha(T_v),$$ensuring that further splits are retained only when they yield sufficient reduction in classification error to justify the added complexity.

```{=latex}
\begin{algorithm}
\caption{Decision Tree with Integrated Cost-Complexity Pruning}
\label{alg:custom-tree}
\begin{algorithmic}[1]
\STATE \textbf{Procedure} GenerateTree($\mathcal{Y}, \mathbf{X}, d, d_{\max}, \alpha$)
    \STATE $n \gets |\mathcal{Y}|$
    \STATE $\mathbf{p} \gets \text{ClassProbabilities}(\mathcal{Y})$
    \STATE $\hat{y} \gets \arg\max_c p_c$
    
    \IF{$d \geq d_{\max}$ \textbf{or} $|\text{unique}(\mathcal{Y})| \leq 1$}
        \RETURN $\text{LeafNode}(\hat{y}, \mathbf{p}, n)$
    \ENDIF
    
    \STATE $(f^*, s^*, \text{score}) \gets \text{FeatureSelector}(\mathcal{Y}, \mathbf{X}, \text{criterion})$
    
    \IF{$f^* = \text{null}$ \textbf{or} $\text{score} = -\infty$}
        \RETURN $\text{LeafNode}(\hat{y}, \mathbf{p}, n)$
    \ENDIF
    
    \STATE $\mathcal{I}_L, \mathcal{I}_R \gets \text{Split}(\mathbf{X}[f^*], s^*)$
    
    \IF{$|\mathcal{I}_L| = 0$ \textbf{or} $|\mathcal{I}_R| = 0$}
        \RETURN $\text{LeafNode}(\hat{y}, \mathbf{p}, n)$
    \ENDIF
    
    \STATE $T_L \gets \textsc{GenerateTree}(\mathcal{Y}[\mathcal{I}_L], \mathbf{X}[\mathcal{I}_L], d+1, d_{\max}, \alpha)$
    \STATE $T_R \gets \textsc{GenerateTree}(\mathcal{Y}[\mathcal{I}_R], \mathbf{X}[\mathcal{I}_R], d+1, d_{\max}, \alpha)$
    
    \STATE $T_v \gets \text{InternalNode}(f^*, s^*, T_L, T_R, \hat{y}, \mathbf{p}, n)$
    
    \STATE \COMMENT{Integrated pruning decision}
    \STATE $R_{\text{subtree}} \gets \text{SubtreeError}(T_v) + \alpha \cdot \text{CountLeaves}(T_v)$
    \STATE $R_{\text{leaf}} \gets (1 - \max(\mathbf{p})) + \alpha$
    
    \IF{$R_{\text{leaf}} \leq R_{\text{subtree}}$}
        \RETURN $\text{LeafNode}(\hat{y}, \mathbf{p}, n)$ \COMMENT{Prune subtree}
    \ELSE
        \RETURN $T_v$
    \ENDIF
\STATE \textbf{End Procedure}
\end{algorithmic}
\end{algorithm}
```

The custom uni variate decision tree implementation in R combines axis-parallel splits with integrated cost-complexity pruning, offering several key differences from standard CART/\CRANpkg{rpart} trees. Numeric features are split only at midpoints where class labels change, while categorical features use binary splits at each level, reducing computational complexity. Splitting criteria include Gini impurity, information gain, and Gain Ratio—the latter penalizing splits with high split information to favor balanced partitions. Integrated pruning is applied during tree construction using a fixed $\alpha$ parameter, replacing sub-trees with leaves whenever the cost-complexity criterion is satisfied, unlike CART/\CRANpkg{rpart}, which performs post-pruning via cross-validation. This single-pass pruning approach reduces training time and space requirements, while still producing interpret-able trees. Predictions follow standard tree traversal, returning majority-class labels or probability estimates at leaves.

## Support Vector Machine based Oblique Decision Trees

### Algorithm Description

The SVM Oblique Decision Tree (SVMODT) constructs a binary classification decision tree where each internal node $v$ at depth $d$. The algorithm is implemented in R using the \CRANpkg{e1071} performs the following operations:

1.  **Feature Selection:** A subset of $m_d$ features is dynamically selected from the available feature set $\mathcal{F}$. This selection can be done using one of several strategies, including random selection, sampling, mutual information ranking (using the \CRANpkg{FSelectorRcpp} package), or correlation-based selection. This allows the tree to adaptive-ly focus on the most informative features at each node.

2.  **Feature Scaling:** Once the features are selected, z-score normalization is applied to standardize them. For the selected feature matrix $\mathbf{X}_v \in \mathbb{R}^{n \times m_d}$, the scaled features are computed as $$\mathbf{X}_v^{\text{scaled}} = \frac{\mathbf{X}_v - \boldsymbol{\mu}_v}{\boldsymbol{\sigma}_v},$$ where $\boldsymbol{\mu}_v$ and $\boldsymbol{\sigma}_v$ denote the mean and standard deviation of the features in node $v$.

3.  **SVM Training:** A linear SVM is then trained on the scaled features. Optional class weights $w_c$ can be applied to handle imbalanced data. The SVM optimization problem is formulated as $$\min_{\mathbf{w}, b} \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^{n} w_{y_i} \xi_i,$$ where $C$ is the regularization parameter, $\xi_i$ are slack variables, and $w_{y_i}$ represents the class-specific weight for sample $i$.

4.  **Node Splitting:** After training, the decision values for each sample are computed as $f(x_i) = \mathbf{w}^T x_i + b.$ Samples are then partitioned into left and right child nodes according to the sign of $f(x_i)$:

    -   Left child: $\{i : f(x_i) > 0\}$

    -   Right child: $\{i : f(x_i) \leq 0\}$

This process recursively continues for each child node until stopping criteria are met.

```{=latex}
\begin{algorithm}
\caption{SVM Oblique Decision Tree Construction}
\label{alg:svmodt}
\begin{algorithmic}[1]
\STATE \textbf{Procedure} SVMSplit($\mathcal{D}, d, d_{\max}, n_{\min}$)
    \STATE $n \gets |\mathcal{D}|$, $\mathcal{Y} \gets \text{labels}(\mathcal{D})$
    
    \IF{$d > d_{\max}$ \textbf{or} $|\text{unique}(\mathcal{Y})| = 1$ \textbf{or} $n < n_{\min}$}
        \RETURN $\text{LeafNode}(\mathcal{Y})$ \COMMENT{Stopping criteria}
    \ENDIF
    
    \STATE $m_d \gets \text{DynamicMaxFeatures}(d, m_{\text{base}}, \text{strategy})$
    \STATE $\mathcal{F}_d \gets \text{SelectFeatures}(\mathcal{D}, m_d, \text{method})$
    \STATE $\mathbf{X}_{\text{scaled}}, \text{scaler} \gets \text{Scale}(\mathcal{D}[\mathcal{F}_d])$
    
    \IF{$|\mathcal{F}_d| = 0$}
        \RETURN $\text{LeafNode}(\mathcal{Y})$ \COMMENT{No valid features}
    \ENDIF
    
    \STATE $w_c \gets \text{CalculateClassWeights}(\mathcal{Y}, \text{strategy})$
    \STATE $\text{SVM} \gets \text{FitLinearSVM}(\mathbf{X}_{\text{scaled}}, \mathcal{Y}, w_c)$
    
    \IF{$\text{SVM} = \text{null}$}
        \RETURN $\text{LeafNode}(\mathcal{Y})$ \COMMENT{SVM fitting failed}
    \ENDIF
    
    \STATE $\mathbf{f} \gets \text{SVM.DecisionValues}(\mathbf{X}_{\text{scaled}})$
    \STATE $\mathcal{I}_L \gets \{i : f_i > 0\}$, $\mathcal{I}_R \gets \{i : f_i \leq 0\}$
    
    \IF{$|\mathcal{I}_L| = 0$ \textbf{or} $|\mathcal{I}_R| = 0$}
        \RETURN $\text{LeafNode}(\mathcal{Y})$ \COMMENT{Ineffective split}
    \ENDIF
    
    \STATE $\text{left} \gets \textsc{SVMSplit}(\mathcal{D}[\mathcal{I}_L], d+1, d_{\max}, n_{\min})$
    \STATE $\text{right} \gets \textsc{SVMSplit}(\mathcal{D}[\mathcal{I}_R], d+1, d_{\max}, n_{\min})$
    
    \RETURN $\text{InternalNode}(\text{SVM}, \mathcal{F}_d, \text{scaler}, \text{left}, \text{right})$
\STATE \textbf{End Procedure}
\end{algorithmic}
\end{algorithm}
```

### Key Hyper-parameters

1.  **Feature Selection:** The algorithm supports three strategies for selecting features at each node. The maximum number of features considered at depth $d$ is defined as:

$$
m_d =
\begin{cases}
m_{\text{base}} & \text{constant strategy} \\
\lfloor m_{\text{base}} \cdot \alpha^{d-1} \rfloor & \text{decrease strategy} \\
\text{Uniform}(\lfloor p \cdot \ell_{\min} \rfloor, \lfloor p \cdot \ell_{\max} \rfloor) & \text{random strategy}
\end{cases}
$$

Here, $\alpha \in (0, 1]$ is the decrease rate for the "decrease strategy," $p$ is the total number of features, $\ell_{\min}$ and $\ell_{\max}$ define the fractional bounds for the random strategy, and $m_{\text{base}}$ is the base number of features at the first depth.

2.  **Feature Penalty:** To encourage diversity in feature usage across the tree, previously used features have their selection weight reduced by a factor of $(1 - \lambda)$, where $\lambda \in [0, 1)$.

3.  **Class Weight Handling:** To address class imbalance, the algorithm allows four weighting schemes:

    -   **None:** $w_c = 1$ for all classes.

    -   **Balanced:** $w_c = \frac{n}{K \cdot n_c}$, where $K$ is the total number of classes and $n_c$ is the number of samples in class $c$.

    -   **Balanced sub-sample:** $w_c = \frac{1}{n_c} \cdot \frac{K}{\sum_{c'} 1/n_{c'}}$, which adjusts weights for sub-sampled data.

    -   **Custom:** Users can define their own class weights.

### Interaction with SVM Training and Node Splitting

The hyper-parameters described above directly influence the behavior of the SVM-based decision tree at each node. The feature selection strategy determines which subset of features $\mathcal{F}_d$ is used to fit the linear SVM at depth $d$, which affects the orientation and effectiveness of the decision hyper-plane. Feature penalties ensure that no single feature dominates multiple splits, promoting diversity in the learned splits and improving generalization. Once the SVM is trained on the selected and scaled features, the decision values are used to partition the samples into left and right child nodes. By controlling the number of features, penalizing repeated use, and adjusting class weights, the tree can achieve a balance between predictive accuracy, interpret-ability, and computational efficiency.

# Data

In this study, we evaluate the SVM-based oblique decision tree (SVMODT) on two widely used benchmark datasets: **Palmer Penguins** and **Wisconsin Diagnostic Breast Cancer (WDBC)**. These datasets provide a mix of multi-class and binary classification problems, as well as numeric and categorical features, making them suitable for testing the flexibility and interpretability of SVMODTs.

## Palmer Penguins

The \CRANpkg{palmerpenguins} data set is a multi-class data set containing morphological measurements for three penguin species (*Adelie*, *Chinstrap*, and *Gentoo*) observed on the Palmer Archipelago, Antarctica. The data set includes 344 complete observations with the following features:

-   **bill_length_mm**: Length of the bill (numeric)\
-   **bill_depth_mm**: Depth of the bill (numeric)\
-   **flipper_length_mm**: Length of the flipper (numeric)\
-   **body_mass_g**: Body mass (numeric)\
-   **sex**: Sex of the penguin (categorical)\
-   **species**: Target class (categorical with three levels)

## Wisconsin Diagnostic Breast Cancer (WDBC)

The **WDBC** [@Street1993] data set is a binary classification data set derived from fine needle aspirates of breast tissue. It contains 569 observations with 30 numeric features computed from digitized images of cell nuclei. The features include measures such as **radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension** The target variable is **diagnosis** (Malignant (M) or benign (B) tumor). This data set is widely used for testing high-dimensional binary classification algorithms. Its numeric nature allows direct evaluation of SVM splits and the effect of class weighting strategies on imbalanced classes.

## Data Preparation

For both data sets, the following pre-processing steps are applied before training the SVMODT:

1.  **Missing Values**: Rows with missing values are removed.
2.  **Class Filtering**: For Palmer Penguins, rows corresponding to the *Gentoo* species are removed using \CRANpkg{dplyr} to create a binary classification problem.
3.  **Categorical Encoding**: Categorical variables are encoded as factor levels suitable for training.

These pre-processing steps ensure that SVMODT can learn meaningful oblique splits across both data sets while maintaining interpret-ability.

# Usage and Examples

## Custom Univariate Decision Tree

The custom decision tree implementation provides a flexible framework for building axis-aligned decision trees with integrated cost-complexity pruning. This section demonstrates the usage of the `generate_tree()` function through practical examples.

## Basic Usage

The primary function `generate_tree()` constructs a decision tree using the following key parameters:

-   **target**: A vector of class labels (factor or character)

-   **features**: A data frame containing predictor variables

-   **criteria_type**: Splitting criterion (`"gini"`, `"info_gain"`, `"gain_ratio"`)

-   **ig_metric**: Impurity metric for information gain (`"gini"` or `"entropy"`)

-   **max_depth**: Maximum tree depth (controls model complexity)

-   **alpha**: Cost-complexity parameter for integrated pruning ($\alpha \geq 0$)

### Example 1: Palmer Penguins with Gini Impurity

In this example, we construct an oblique decision tree using the function `generate_tree()`. The splitting criterion is set to **Gini impurity**, with a maximum tree depth of 4. We also specify a cost-complexity pruning parameter `$\alpha = 0.01$` to control sub-tree pruning during tree construction, ensuring a balance between model complexity and generalization performance.

```{r}
library(palmerpenguins)
library(dplyr)

# Prepare data
penguins_data <- penguins |>
  filter(species %in% c("Adelie", "Chinstrap")) |>
  select(species, bill_length_mm, bill_depth_mm, 
         flipper_length_mm, body_mass_g) |>
  na.omit() |>
  mutate(species = droplevels(species))

# Split into training and test sets
set.seed(234)
train_idx <- sample(nrow(penguins_data), 0.8 * nrow(penguins_data))
train_data <- penguins_data[train_idx, ]
test_data <- penguins_data[-train_idx, ]
```

```{r}
# Train custom tree with Gini impurity
tree_gini <- svmodt:::generate_tree(
  target = train_data$species,
  features = train_data[, -1],  # Exclude response
  criteria_type = "gini",
  max_depth = 4,
  alpha = 0.01  # Cost-complexity parameter
)

svmodt:::print_tree(tree_gini)
```

The output shows a hierarchical structure where each internal node displays the splitting feature, threshold value, and sample count. For example, a node showing `bill_length_mm <= 44.65 (n = 175)` indicates that samples with bill length less than or equal to 44.65 mm go to the left child, while others go right. Leaf nodes display the predicted class and sample count.

### Example 2: Comparing Splitting Criteria with Breast Cancer Data

To compare different splitting criteria, we train three decision trees on the data set (predicting `diagnosis`). First, a tree using **Gini impurity** is trained with a maximum depth of 4 and a cost-complexity pruning parameter `$\alpha = 0.01$`. Next, an **Information Gain**-based tree is trained using the entropy metric to evaluate splits. Finally, a tree using **Gain Ratio** is constructed, also based on entropy, which normalizes information gain by split information to reduce bias toward features with many levels.

```{r}
# Split into training and test sets
set.seed(234)
train_idx <- sample(nrow(wdbc), 0.8 * nrow(wdbc))
train_data <- wdbc[train_idx, ]
test_data <- wdbc[-train_idx, ]
```

```{r echo=TRUE}
set.seed(123)

# Train with Information Gain (using entropy)
tree_gini <- svmodt:::generate_tree(
  target = train_data$diagnosis,
  features = train_data[, -1],
  criteria_type = "gini",
  max_depth = 4,
  alpha = 0.01
)

tree_ig <- svmodt:::generate_tree(
  target = train_data$diagnosis,
  features = train_data[, -1],
  criteria_type = "info_gain",
  ig_metric = "entropy",
  max_depth = 4,
  alpha = 0.01
)

# Train with Gain Ratio
tree_gr <- svmodt:::generate_tree(
  target = train_data$diagnosis,
  features = train_data[, -1],
  criteria_type = "gain_ratio",
  ig_metric = "entropy",
  max_depth = 4,
  alpha = 0.01
)
```

```{r}
# Compare tree sizes and accuracies
compare_trees <- data.frame(
  Criterion = c("Gini", "Info Gain", "Gain Ratio"),
  Num_Leaves = c(
    svmodt:::count_leaves(tree_gini),
    svmodt:::count_leaves(tree_ig),
    svmodt:::count_leaves(tree_gr)
  ),
  Test_Accuracy = c(
    mean(svmodt:::predict_tree(tree_gini, test_data[, -1]) == test_data$diagnosis),
    mean(svmodt:::predict_tree(tree_ig, test_data[, -1]) == test_data$diagnosis),
    mean(svmodt:::predict_tree(tree_gr, test_data[, -1]) == test_data$diagnosis)
  )
)
```

```{r compare-custom-tree-latex, eval = knitr::is_latex_output()}
knitr::kable(compare_trees, digits = 3, 
             caption = "Comparison of splitting criteria on penguin classification.", format = "latex") |> 
  kable_styling(full_width = FALSE, position = "center", font_size = 7)
```

```{r compare-custom-tree-html, eval = knitr::is_html_output()}
knitr::kable(compare_trees, digits = 3, 
             caption = "Comparison of splitting criteria on penguin classification.", format = "html") |>
  kable_styling(full_width = FALSE, position = "center")
```

Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:compare-custom-tree-html)', '\\@ref(tab:compare-custom-tree-latex)'))` shows the impact of different splitting criteria on the classification of Palmer Penguins. Using the custom decision tree, we compared Gini impurity, information gain, and gain ratio as criteria for feature selection at each node. Both Gini impurity and information gain resulted in trees with six leaves, achieving a test accuracy of 93.9%. In contrast, gain ratio produced a smaller tree with only three leaves while achieving higher test accuracy of 96.5%, demonstrating its ability to favor balanced and informative splits, reduce overfitting, and improve generalization performance.

## Support Vector Machine based Oblique Decision Trees

The svmodt package provides an intuitive interface for building SVM-based oblique decision trees. This section demonstrates the core functionality through two practical examples: ecological species classification and medical diagnosis.

### Basic Usage

The primary function **`svm_split()`** constructs an oblique decision tree using the following key parameters:

-   **data**: A data frame containing predictors and response

-   response: Name of the response variable (as a string)

-   **max_depth**: Maximum tree depth (controls model complexity)

-   **min_samples**: Minimum samples required to split a node

-   **max_features**: Maximum features to consider at each split

-   **feature_method**: Feature selection strategy ("random", "mutual", "cor")

-   **class_weights**: Strategy for handling class imbalance

### Example 1: Penguin Species Classification

We demonstrate the package using the \CRANpkg{palmerpenguins} data set, which contains morphological measurements for three penguin species. We focus on classifying Adelie and Chinstrap penguins.

```{r palmer-data, eval=TRUE}
# Prepare data
penguins_data <- penguins |>
  filter(species %in% c("Adelie", "Chinstrap")) |>
  select(species, bill_length_mm, bill_depth_mm, 
         flipper_length_mm, body_mass_g) |>
  na.omit() |>
  mutate(species = droplevels(species))

# Split data
set.seed(234)
train_idx <- sample(nrow(penguins_data), 0.8 * nrow(penguins_data))
train_data <- penguins_data[train_idx, ]
test_data <- penguins_data[-train_idx, ]
```

#### Model Training

In this example, the model is trained on the `train_data` data set to predict the categorical response variable, *species*. The function `svm_split()` recursively partitions the data using **linear SVM hyper-planes** at each internal node rather than conventional univariate thresholds, thereby producing **oblique splits** capable of capturing multivariate relationships among predictors. The argument `max_depth = 3` constrains the tree to a maximum of three levels, providing control over model complexity and mitigating overfitting. Similarly, `max_features = 2` restricts each node to consider only two features when fitting the SVM, enhancing computational efficiency and interpretability. The parameter `feature_method = "mutual"` specifies that feature selection at each node is based on **mutual information**, ensuring that the most informative variables relative to the response are prioritized. Setting `verbose = FALSE` suppresses intermediate output for streamlined execution.

```{r palmer-model-train, echo=TRUE, results='asis'}
# Train SVMODT with mutual information feature selection
tree <- svm_split(
  data = train_data,
  response = "species",
  max_depth = 3,
  max_features = 2,
  feature_method = "mutual",
  verbose = FALSE
)
```

#### Model Structure

Each internal node represents a **binary linear decision boundary** obtained from a fitted SVM model using the features listed at that node. For example, the root node (depth = 1) uses `bill_length_mm` and `flipper_length_mm` to form the first separating hyper-plane. Observations satisfying $SVM>0$ proceed to the left child, while those with $SVM \leq 0$ move to the right branch.

```{r palemr-model-print}
print_svm_tree(tree, show_penalties = FALSE)
```

Terminal nodes (labeled as *Leaf*) correspond to final class predictions. In this case, most observations in the left sub-tree are classified as *Adelie*, while the right sub-tree primarily identifies *Chinstrap* penguins. The structure illustrates how **SVM-based oblique splits** allow the tree to capture complex linear interactions between multiple predictors—something that traditional axis-aligned decision trees cannot achieve.

#### Visualizing Decision Boundaries

The package includes visualization tools to examine the SVM hyper-planes at each node, allowing users to interpret how features contribute to class separation within the tree. *Note that visualization is currently supported only for trees where each node consistently uses exactly two features,* ensuring that the separating hyper-plane can be rendered clearly in two-dimensional feature space. The `visualize_svm_tree()` function produces a graphical representation of the overall decision hierarchy

```{r penguins-depth-one, results='asis', fig.cap="Visualization of the root node (depth = 1) of the SVM-based oblique decision tree."}
viz <- visualize_svm_tree(
  tree = tree,
  original_data = train_data,
  response_col = "species",
  max_depth = 2
)

# Display root node boundary
viz$plots$depth_1_Root
```

Fig \@ref(fig:penguins-depth-one) depicts the root node of the SVM-based oblique decision tree, where the first split is performed using the features `bill_length_mm` and `flipper_length_mm`. The figure illustrates how the linear SVM at the root node divides the dataset into two branches, guiding samples toward subsequent child nodes. **Note:** This visualization is only possible because the node considers exactly two features, a requirement for 2D plotting.

```{r penguins-depth-two, results='asis', fig.align='center', fig.cap="Visualization of a node (depth = 2)."}
gridExtra::grid.arrange(viz$plots$`depth_2_Root_→_L`, viz$plots$`depth_2_Root_→_R`, ncol = 2)
```

Similarly, Fig \@ref(fig:penguins-depth-two) shows a node at depth 2 of the tree, where the SVM uses bill_length_mm and bill_depth_mm to further partition the data. The decision hyper-plane and sample positions are plotted, highlighting how oblique splits can capture multivariate relationships that univariate thresholds cannot. This figure demonstrates the recursive nature of the tree and the refinement of class separation at deeper levels. As with the root node, the visualization is restricted to nodes consistently using exactly two features.

#### Prediction Path Tracing

The `svmodt` package allows tracing the prediction path of individual observations through the SVM-based oblique decision tree. For each sample, the relevant feature values are evaluated at each node using the linear SVM hyper-plane associated with that node. For instance, a sample with `bill_length_mm = 39.5`, `bill_depth_mm = 17.4`, `flipper_length_mm = 186`, and `body_mass_g = 3800` first reaches the root node, which considers `bill_length_mm` and `flipper_length_mm`. The SVM decision value at this node is `2.1098`, which directs the sample along the left branch. At depth 2, the node evaluates `bill_length_mm` and `bill_depth_mm` and produces a decision value of `1.6592`, again guiding the sample to the left branch. Finally, the sample reaches a leaf node that predicts the class `Adelie`, which contains 123 training samples. The complete traversal path is `LEFT -> LEFT`, resulting in the final prediction of `Adelie`. This tracing functionality provides interpret-able insights into how each decision is made within the oblique decision tree.

```{r}
trace_prediction_path(tree, test_data, sample_idx = 1)
```

### Example 2: Breast Cancer Diagnosis

```{r}
data("wdbc", package = "svmodt")

# Split data
set.seed(234)
train_idx <- sample(nrow(wdbc), 0.8 * nrow(wdbc))
train_wdbc <- wdbc[train_idx, ]
test_wdbc <- wdbc[-train_idx, ]
```

We now demonstrate the package on the Wisconsin Diagnostic Breast Cancer (WDBC) data set, highlighting advanced functionalities such as handling class imbalance, feature sub-setting, and feature penalization. These features allow the SVM-based oblique decision tree to effectively manage real-world challenges, including unequal class distributions, high-dimensional feature spaces, and repeated feature usage, while maintaining interpret-ability and predictive performance.

#### Handling Class Imbalance

The package provides built-in support for handling class imbalance through configurable class weighting. In the first example, the `svm_split()` function is applied to the WDBC data set to predict the binary response variable with `class_weights = "balanced"`, which automatically adjusts the weight of each class inversely proportional to its frequency. This ensures that minority classes receive higher influence during SVM training, improving predictive performance on imbalanced data sets.

```{r, echo=TRUE}
# Train with balanced class weights
tree_balanced <- svm_split(
  data = train_wdbc,
  response = "diagnosis",
  max_depth = 4,
  max_features = 5,
  feature_method = "mutual",
  class_weights = "balanced",  # Automatic balancing
  verbose = FALSE
)

print_svm_tree(tree = tree_balanced, 
               show_feature_info = FALSE, 
               show_penalties = FALSE, show_probabilities = TRUE)
```

In the second example, custom class weights can be specified via the `custom_class_weights` argument, assigning a higher penalty to the malignant class (`"M" = 3`) relative to the benign class (`"B" = 1`) to emphasize minimizing false negatives. The resulting trees (`tree_balanced` and `tree_custom`) incorporate these weights during the recursive SVM-based splits, enabling more equitable class separation while still leveraging the oblique decision tree structure.

```{r echo=TRUE}
# Custom class weights for domain-specific costs
custom_weights <- c("B" = 1, "M" = 3)  # Penalize false negatives
tree_custom <- svm_split(
  data = train_wdbc,
  response = "diagnosis",
  max_depth = 4,
  max_features = 5,
  class_weights = "custom",
  custom_class_weights = custom_weights,
  verbose = FALSE
)

print_svm_tree(tree = tree_custom, 
               show_feature_info = FALSE, 
               show_penalties = FALSE, 
               show_probabilities = FALSE)
```

#### Feature Penalization

To encourage feature diversity across the tree, the package implements a feature penalization mechanism that reduces the likelihood of reusing features selected at ancestor nodes.

```{r echo=TRUE}
set.seed(123)
tree_penalty <- svm_split(
  data = train_wdbc,
  response = "diagnosis",
  max_depth = 4,
  max_features = 4,
  penalize_used_features = TRUE,
  feature_penalty_weight = 0.6,
  verbose = FALSE
)

print_svm_tree(tree_penalty, show_probabilities = FALSE, 
               show_feature_info = TRUE, 
               show_penalties = FALSE)
```

Penalization is controlled via the `feature_penalty_weight` parameter, which scales down the selection probability of previously used features. For instance, setting `feature_penalty_weight = 0.6` reduces the weight of used features by 60%, promoting the consideration of alternative predictors at subsequent splits. This mechanism helps improve model robustness, reduces redundancy in feature usage, and enhances interpret-ability by encouraging the tree to explore diverse feature combinations.

#### Dynamic Feature Selection

The package also supports dynamic feature selection strategies, which vary the number of features considered at each node depending on tree depth or randomness.

```{r echo=TRUE}
tree_decrease <- svm_split(
  data = train_wdbc,
  response = "diagnosis", 
  feature_method = "mutual",
  max_depth = 3,
  max_features = 4,
  max_features_strategy = "decrease",
  max_features_decrease_rate = 0.5,
  verbose = FALSE
)

print_svm_tree(tree_decrease, show_penalties = FALSE, 
               show_feature_info = TRUE)
```

In the **decrease strategy**, the number of candidate features at depth $d$ is scaled by a decay factor $\alpha$ , e.g., $m_0 = m_0 \cdot \alpha^{d-1}$ , reducing feature count at deeper levels to improve efficiency and prevent over-fitting.

```{r echo=TRUE}
set.seed(123)

tree_random <- svm_split(
  data = train_wdbc,
  response = "diagnosis",
  feature_method = "mutual",
  max_depth = 4,
  max_features_strategy = "random",
  max_features_random_range = c(0.05, 0.2),  # 30-80% of features
  verbose = FALSE
)

print_svm_tree(tree_random, show_penalties = FALSE, 
               show_feature_info = TRUE)
```

Alternatively, the **random strategy** selects a feature count uniformly at random from a specified range (e.g., 5–20% of the total features) at each node, promoting stochasticity and diversity in splits. These mechanisms provide flexible control over tree complexity and feature exploration, particularly useful in high-dimensional data sets such as the WDBC data set.

# Model Comparison

To evaluate the performance of our proposed methods, we conduct a comprehensive comparison study using the Wisconsin Diagnostic Breast Cancer (WDBC) data set. We compare five classification approaches:

1.  **Custom Decision Tree** - Our implementation with integrated cost-complexity pruning

2.  **SVMODT** - SVM-based oblique decision tree with dynamic feature selection

3.  **CART (rpart)** - Standard axis-aligned decision tree

4.  **Linear SVM** - Support Vector Machine with linear kernel

5.  **RBF SVM** - Support Vector Machine with radial basis function kernel

6.  **Logistic Regression** - Generalized linear model for binary classification

## Experimental Setup

To ensure robust performance estimates, we employ a repeated random sub-sampling validation strategy. For each iteration:

1.  The WDBC data set is randomly split into 80% training and 20% testing sets, stratified by the `diagnosis` variable

2.  Each model is trained on the training set with fixed hyper-parameters

3.  Test set accuracy is recorded

4.  This process is repeated 50 times with different random seeds

The hyper-parameters for each model are configured as follows:

1.  **Custom Tree**: Gini impurity, `max_depth = 4`, `alpha = 0.01`

2.  **SVMODT**: Mutual information feature selection, max_depth = 4, decreasing feature strategy with rate 0.5, feature penalization enabled

3.  **CART**: Complexity parameter `cp = 0.01`

4.  **Linear SVM**: Default regularization, linear kernel

5.  **RBF SVM**: Default regularization, RBF kernel with automatic gamma selection

6.  **Logistic Regression**: L2 regularization with default parameters

```{r cache=TRUE, results='hide'}
set.seed(123)
n_iter <- 100

results <- data.frame(
  Custom_Tree = numeric(n_iter),
  SVMODT = numeric(n_iter),
  RPART = numeric(n_iter),
  Linear_SVM = numeric(n_iter),
  RBF_SVM = numeric(n_iter),
  Logistic = numeric(n_iter)
)

for (i in 1:n_iter) {
  # Stratified train-test split
  split_data <- initial_split(wdbc, prop = 0.8, strata = diagnosis)
  train_data <- training(split_data)
  test_data <- testing(split_data)
  
  # Separate features and response
  x_train <- train_data[, names(wdbc) != "diagnosis"]
  y_train <- train_data$diagnosis
  x_test <- test_data[, names(wdbc) != "diagnosis"]
  y_test <- test_data$diagnosis
  
  #------------------- Custom Decision Tree -------------------#
  custom_tree <- svmodt:::generate_tree(
    target = y_train,
    features = x_train,
    criteria_type = "gini",
    max_depth = 4,
    alpha = 0.01
  )
  pred_custom <- svmodt:::predict_tree(custom_tree, x_test)
  results$Custom_Tree[i] <- mean(pred_custom == y_test)
  
  #------------------- SVMODT -------------------#
  svmodt_tree <- svm_split(
    data = train_data,
    response = "diagnosis",
    max_depth = 4,
    max_features = 29,
    feature_method = "mutual",
    max_features_strategy = "decrease",
    max_features_decrease_rate = 0.5,
    penalize_used_features = TRUE,
    verbose = FALSE
  )
  pred_svmodt <- svm_predict_tree(svmodt_tree, test_data)
  results$SVMODT[i] <- mean(pred_svmodt == y_test)
  
  #------------------- RPART -------------------#
  rpart_model <- rpart(
    formula = diagnosis ~ .,
    data = train_data,
    control = rpart.control(cp = 0.01)
  )
  pred_rpart <- predict(rpart_model, test_data, type = "class")
  results$RPART[i] <- mean(pred_rpart == y_test)
  
  #------------------- Linear SVM -------------------#
  linear_svm <- ksvm(
    diagnosis ~ .,
    data = train_data,
    kernel = "vanilladot",
    prob.model = FALSE
  )
  pred_linear <- predict(linear_svm, test_data)
  results$Linear_SVM[i] <- mean(pred_linear == y_test)
  
  #------------------- RBF SVM -------------------#
  rbf_svm <- ksvm(
    diagnosis ~ .,
    data = train_data,
    kernel = "rbfdot",
    prob.model = FALSE
  )
  pred_rbf <- predict(rbf_svm, test_data)
  results$RBF_SVM[i] <- mean(pred_rbf == y_test)
  
  #------------------- Logistic Regression -------------------#
  logistic_model <- glm(
    diagnosis ~ .,
    data = train_data,
    family = binomial(link = "logit")
  )
  pred_logistic_prob <- predict(logistic_model, test_data, type = "response")
  pred_logistic <- ifelse(pred_logistic_prob > 0.5, "M", "B")
  pred_logistic <- factor(pred_logistic, levels = levels(y_test))
  results$Logistic[i] <- mean(pred_logistic == y_test)
}
```

## Summary Statistics

Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:summary-stats-html)', '\\@ref(tab:summary-stats-latex)'))` presents the mean accuracy and standard deviation across 50 iterations for each model.

```{r}
# Calculate summary statistics
summary_stats <- results %>%
  summarise(across(everything(), list(
    Mean = ~mean(.x),
    SD = ~sd(.x),
    Min = ~min(.x),
    Max = ~max(.x)
  )))

# Reshape for better presentation
summary_table <- data.frame(
  Model = c("Custom Tree", "SVMODT", "RPART", "Linear SVM", "RBF SVM", "Logistic Regression"),
  Mean = c(
    mean(results$Custom_Tree),
    mean(results$SVMODT),
    mean(results$RPART),
    mean(results$Linear_SVM),
    mean(results$RBF_SVM),
    mean(results$Logistic)
  ),
  SD = c(
    sd(results$Custom_Tree),
    sd(results$SVMODT),
    sd(results$RPART),
    sd(results$Linear_SVM),
    sd(results$RBF_SVM),
    sd(results$Logistic)
  ),
  Min = c(
    min(results$Custom_Tree),
    min(results$SVMODT),
    min(results$RPART),
    min(results$Linear_SVM),
    min(results$RBF_SVM),
    min(results$Logistic)
  ),
  Max = c(
    max(results$Custom_Tree),
    max(results$SVMODT),
    max(results$RPART),
    max(results$Linear_SVM),
    max(results$RBF_SVM),
    max(results$Logistic)
  )
)
```

```{r summary-stats-latex, eval = knitr::is_latex_output()}
kable(summary_table, 
      digits = 4,
      col.names = c("Model", "Mean", "SD", "Min", "Max"),
      format = "latex",
      caption = "Summary statistics of test accuracy across 50 iterations on WDBC dataset") |>
  kable_styling(full_width = FALSE, position = "center")
```

```{r summary-stats-html, eval = knitr::is_html_output()}
kable(summary_table, 
      digits = 4,
      col.names = c("Model", "Mean", "SD", "Min", "Max"),
      format = "html",
      caption = "Summary statistics of test accuracy across 50 iterations on WDBC dataset") |>
  kable_styling(full_width = FALSE, position = "center")
```

The **SVM-based Oblique Decision Tree (SVMODT)** achieved a mean test accuracy of 0.9737 with a standard deviation of 0.0146, demonstrating both high predictive performance and low variability. Traditional decision trees trained using **rpart** yielded a lower mean accuracy of 0.9289, reflecting limitations of axis-aligned splits. A **custom cost-complexity tree** achieved a mean accuracy of 0.9323, slightly higher than rpart but still below SVMODT. Among standard SVM classifiers, the **linear SVM** achieved 0.9730 and the **RBF kernel SVM** slightly outperformed with 0.9745 mean accuracy. **Logistic regression** provided competitive results (mean 0.9450) but lagged behind kernel-based methods.

## Visualization of Results

### Accuracy Distribution

Fig \@ref(fig:boxplot-comparison) shows the distribution of test accuracy across all iterations for each model.

```{r boxplot-comparison, results='asis', fig.cap="Model Accuracy Distribution (50 Resamples)"}
# Reshape data for plotting
results_long <- tidyr::pivot_longer(
  results,
  cols = everything(),
  names_to = "Model",
  values_to = "Accuracy"
)

# Rename models for better display
results_long <- results_long %>%
  mutate(Model = recode(Model,
    "Custom_Tree" = "Custom Tree\n(Integrated Pruning)",
    "SVMODT" = "SVMODT\n(Oblique Splits)",
    "RPART" = "RPART\n(CART)",
    "Linear_SVM" = "Linear SVM",
    "RBF_SVM" = "RBF SVM",
    "Logistic" = "Logistic\nRegression"
  ))

# Create boxplot
ggplot(results_long, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_boxplot(alpha = 0.7, outlier.shape = 1) +
  stat_summary(fun = mean, geom = "point", shape = 23, size = 3, fill = "white") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set2") +
  labs(
    y = "Test Accuracy",
    x = ""
  ) +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5)
  ) +
  coord_cartesian(ylim = c(0.85, 1.0))
```

### Pairwise Comparisons

We perform pairwise comparisons between our custom tree implementation and RPART, as well as between SVMODT and standard SVMs.

#### Custom Tree vs. RPART

Fig \@ref(fig:custom-vs-rpart) presents Bland-Altman plots comparing the custom decision tree with RPART.

```{r custom-vs-rpart, results='asis', fig.cap="Custom Tree vs. RPART Comparison"}
# Prepare data for Bland-Altman plots
ba_data <- data.frame(
  custom = 1 - results$Custom_Tree,  # Convert to error rate
  rpart = 1 - results$RPART
)

# Calculate statistics
mean_diff <- mean(ba_data$custom - ba_data$rpart)
sd_diff <- sd(ba_data$custom - ba_data$rpart)
upper_loa <- mean_diff + 1.96 * sd_diff
lower_loa <- mean_diff - 1.96 * sd_diff

# Scatter plot
p1 <- ggplot(ba_data, aes(x = rpart, y = custom)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(
    x = "RPART Prediction Error",
    y = "Custom Tree Prediction Error",
    title = "Scatter Plot"
  ) +
  theme_minimal() +
  theme(aspect.ratio = 1) +
  coord_fixed(ratio = 1)

# Bland-Altman plot
p2 <- ggplot(ba_data, aes(x = (custom + rpart) / 2, y = custom - rpart)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_hline(yintercept = mean_diff, color = "blue", linewidth = 1) +
  geom_hline(yintercept = upper_loa, color = "green", linewidth = 1, linetype = "dashed") +
  geom_hline(yintercept = lower_loa, color = "green", linewidth = 1, linetype = "dashed") +
  annotate("text", x = Inf, y = mean_diff, label = sprintf("Mean: %.4f", mean_diff), 
           hjust = 1.1, vjust = -0.5, color = "blue") +
  annotate("text", x = Inf, y = upper_loa, label = sprintf("+1.96 SD: %.4f", upper_loa), 
           hjust = 1.1, vjust = -0.5, color = "green") +
  annotate("text", x = Inf, y = lower_loa, label = sprintf("-1.96 SD: %.4f", lower_loa), 
           hjust = 1.1, vjust = 1.5, color = "green") +
  labs(
    x = "Mean Prediction Error",
    y = "Difference (Custom - RPART)",
    title = "Bland-Altman Plot"
  ) +
  theme_minimal() +
  theme(aspect.ratio = 1)

# Combine plots
p1 + p2
```

#### SVMODT vs. Standard SVMs

Fig \@ref(fig:svmodt-vs-svm) compares SVMODT against Linear and RBF SVMs.

```{r svmodt-vs-svm, results='asis', fig.cap="SVMODT vs. Linear SVM Comparison"}
# SVMODT vs Linear SVM
ba_data_svm <- data.frame(
  svmodt = 1 - results$SVMODT,
  linear_svm = 1 - results$Linear_SVM
)

mean_diff_svm <- mean(ba_data_svm$svmodt - ba_data_svm$linear_svm)
sd_diff_svm <- sd(ba_data_svm$svmodt - ba_data_svm$linear_svm)
upper_loa_svm <- mean_diff_svm + 1.96 * sd_diff_svm
lower_loa_svm <- mean_diff_svm - 1.96 * sd_diff_svm

p3 <- ggplot(ba_data_svm, aes(x = linear_svm, y = svmodt)) +
  geom_point(alpha = 0.6, color = "coral") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(
    x = "Linear SVM Prediction Error",
    y = "SVMODT Prediction Error",
    title = "Scatter Plot"
  ) +
  theme_minimal() +
  theme(aspect.ratio = 1) +
  coord_fixed(ratio = 1)

p4 <- ggplot(ba_data_svm, aes(x = (svmodt + linear_svm) / 2, y = svmodt - linear_svm)) +
  geom_point(alpha = 0.6, color = "coral") +
  geom_hline(yintercept = mean_diff_svm, color = "blue", linewidth = 1) +
  geom_hline(yintercept = upper_loa_svm, color = "green", linewidth = 1, linetype = "dashed") +
  geom_hline(yintercept = lower_loa_svm, color = "green", linewidth = 1, linetype = "dashed") +
  annotate("text", x = Inf, y = mean_diff_svm, label = sprintf("Mean: %.4f", mean_diff_svm), 
           hjust = 1.1, vjust = -0.5, color = "blue") +
  annotate("text", x = Inf, y = upper_loa_svm, label = sprintf("+1.96 SD: %.4f", upper_loa_svm), 
           hjust = 1.1, vjust = -0.5, color = "green") +
  annotate("text", x = Inf, y = lower_loa_svm, label = sprintf("-1.96 SD: %.4f", lower_loa_svm), 
           hjust = 1.1, vjust = 1.5, color = "green") +
  labs(
    x = "Mean Prediction Error",
    y = "Difference (SVMODT - Linear SVM)",
    title = "Bland-Altman Plot"
  ) +
  theme_minimal() +
  theme(aspect.ratio = 1)

p3 + p4
```

## Statistical Significance Testing

We conducted paired t-tests to assess whether differences in predictive accuracy between models were statistically significant across the 50 iterations.

```{r}
# Paired t-tests
t_test_results <- data.frame(
  Comparison = character(),
  Mean_Diff = numeric(),
  t_statistic = numeric(),
  p_value = numeric(),
  stringsAsFactors = FALSE
)

comparisons <- list(
  c("Custom_Tree", "RPART", "Custom Tree vs. RPART"),
  c("SVMODT", "Linear_SVM", "SVMODT vs. Linear SVM"),
  c("SVMODT", "RBF_SVM", "SVMODT vs. RBF SVM"),
  c("SVMODT", "Custom_Tree", "SVMODT vs. Custom Tree"),
  c("Custom_Tree", "Logistic", "Custom Tree vs. Logistic Regression"),
  c("SVMODT", "Logistic", "SVMODT vs. Logistic Regression")
)

for (comp in comparisons) {
  test <- t.test(results[[comp[1]]], results[[comp[2]]], paired = TRUE)
  t_test_results <- rbind(t_test_results, data.frame(
    Comparison = comp[3],
    Mean_Diff = mean(results[[comp[1]]] - results[[comp[2]]]),
    t_statistic = test$statistic,
    p_value = test$p.value
  ))
}
```

```{r t-test-results-latex, eval = knitr::is_latex_output()}
kable(t_test_results,
      digits = 4,
      caption = "Paired t-test results for model comparisons. Positive mean differences indicate the first model has higher accuracy.") |>
  kable_styling(full_width = FALSE, position = "center", , font_size = 7)
```

```{r t-test-results-html, eval= knitr::is_html_output()}
kable(t_test_results,
      digits = 4,
      caption = "Paired t-test results for model comparisons. Positive mean differences indicate the first model has higher accuracy.") |>
  kable_styling(full_width = FALSE, position = "center")
```

From Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:t-test-results-html)', '\\@ref(t-test-results-latex)'))` Positive mean differences indicate that the first model in each comparison outperformed the second. The **Custom Tree** achieved a slightly higher mean accuracy than \CRANpkg{rpart} (mean difference = 0.0034, *p* = 0.0362), suggesting a marginal but significant improvement. **SVMODT** showed comparable performance to **Linear SVM** (mean difference = 0.0006, *p* = 0.2880) and **RBF SVM** (mean difference = -0.0009, *p* = 0.4801), indicating no significant difference, while significantly outperforming the **Custom Tree** (mean difference = 0.0414, *p* \< 0.001) and **Logistic Regression** (mean difference = 0.0287, *p* \< 0.001). Conversely, the Custom Tree performed worse than logistic regression (mean difference = -0.0127, *p* \< 0.001).

## Discussion of Results

1.  **Tree-based Methods Performance**: Both the custom decision tree and RPART achieve comparable accuracy, demonstrating that the integrated pruning approach performs competitively with the established CART algorithm. The Bland-Altman analysis in Fig \@ref(fig:custom-vs-rpart) shows that differences between the two methods are generally small and fall within acceptable limits of agreement.
2.  **SVMODT Advantage**: The SVMODT consistently outperforms traditional axis-aligned decision trees, achieving mean accuracy of 0.9737. This improvement demonstrates the benefit of oblique splits in capturing complex decision boundaries that cannot be efficiently represented by axis-parallel partitions. The ability to combine features through linear SVM hyper-planes enables more compact and potentially more generalized tree structures.
3.  **Comparison with Standard SVMs**: SVMODT achieves accuracy comparable to standard Linear SVM while maintaining the interpret ability advantages of tree structures. The RBF SVM shows slightly higher accuracy (mean = 0.9745), which is expected given its ability to model nonlinear decision boundaries. However, this comes at the cost of reduced interpret-ability—the SVMODT offers a transparent decision path that can be visualized and understood, while RBF SVM predictions are essentially black-box.
4.  **Logistic Regression Baseline**: Logistic regression achieves competitive performance (mean accuracy = 0.945), demonstrating that the WDBC data set is largely linearly separable. However, both SVMODT and standard SVMs outperform logistic regression, suggesting that margin-based optimization provides additional robustness.
5.  **Variance in Performance**: All methods show relatively low variance across the 50 re-sampling iterations, indicating stable performance across different train-test splits. The RBF SVM exhibits the lowest variance, followed closely by Linear SVM and SVMODT.
6.  **Integrated Pruning Effectiveness**: The custom tree's competitive performance validates the integrated cost-complexity pruning approach. By making pruning decisions during tree construction rather than as post-processing, the method achieves similar accuracy to CART while potentially offering computational advantages (though timing comparisons were not the focus of this experiment).
