@article{lopez2018,
author = {Rivera-Lopez, Rafael and Canul-Reich, Juana},
year = {2018},
month = {01},
pages = {1-1},
title = {Construction of Near-Optimal Axis-Parallel Decision Trees Using a Differential-Evolution-Based Approach},
volume = {PP},
journal = {IEEE Access},
doi = {10.1109/ACCESS.2017.2788700}
}

@book{breiman1984,
  author = {Breiman, Leo and Friedman, J. H. and Olshen, R. A. and Stone, C. J.},
  publisher = {Wadsworth},
  title = {Classification and Regression Trees.},
  url = {http://lyle.smu.edu/~mhd/8331f06/cart.pdf},
  year = 1984
}

@book{quinlan1993,
  title={C4. 5: programs for machine learning},
  author={Quinlan, J Ross},
  year={2014},
  publisher={Elsevier}
}

@article{mingers1989,
author = {Mingers, John},
year = {1989},
month = {01},
pages = {227-243},
title = {An Empirical Comparison of Pruning Methods for Decision Tree Induction},
volume = {4},
journal = {Machine Learning},
doi = {10.1023/A:1022604100933}
}

@incollection{schaffer1992,
title = {Deconstructing the Digit Recognition Problem},
editor = {Derek Sleeman and Peter Edwards},
booktitle = {Machine Learning Proceedings 1992},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {394-399},
year = {1992},
isbn = {978-1-55860-247-2},
doi = {https://doi.org/10.1016/B978-1-55860-247-2.50056-5},
url = {https://www.sciencedirect.com/science/article/pii/B9781558602472500565},
author = {Cullen Schaffer},
abstract = {Decision tree pruning techniques and other forms of overfitting avoidance have often been considered statistical means of improving predictive accuracy. Intuitively, they are intended to determine the appropriate level of complexity for an induced model by distinguishing between signal and noise in training data, patterns that reflect the true underlying nature of data generation and those that arise by chance. In fact, however, overfitting avoidance methods simply encode preferences for certain classes of models. These preferences may increase accuracy when they bias induction in favor of predictive models, but they may just as well have the opposite effect if they bias induction away from predictive models. This paper analyzes the conditional value of overfitting avoidance and focuses on the effect of one highly-regarded pruning technique in variations of the well-known digit recognition problem.}
}
