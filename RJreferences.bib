@article{lopez2018,
author = {Rivera-Lopez, Rafael and Canul-Reich, Juana},
year = {2018},
month = {01},
pages = {1-1},
title = {Construction of Near-Optimal Axis-Parallel Decision Trees Using a Differential-Evolution-Based Approach},
volume = {PP},
journal = {IEEE Access},
doi = {10.1109/ACCESS.2017.2788700}
}

@article{pagallo1990,
  title={Boolean Feature Discovery in Empirical Learning},
  author={Giulia Pagallo and David Haussler},
  journal={Machine Learning},
  year={1990},
  volume={5},
  pages={71-99},
  url={https://api.semanticscholar.org/CorpusID:5661437}
}

@book{breiman1984,
  author = {Breiman, Leo and Friedman, J. H. and Olshen, R. A. and Stone, C. J.},
  publisher = {Wadsworth},
  title = {Classification and Regression Trees.},
  url = {http://lyle.smu.edu/~mhd/8331f06/cart.pdf},
  year = 1984
}

@article{quinlan1986,
  title={Induction of Decision Trees},
  author={J. Ross Quinlan},
  journal={Machine Learning},
  year={1986},
  volume={1},
  pages={81-106},
  url={https://api.semanticscholar.org/CorpusID:189902138}
}

@book{quinlan1993,
  title={C4. 5: programs for machine learning},
  author={Quinlan, J Ross},
  year={2014},
  publisher={Elsevier}
}

@article{mingers1989,
author = {Mingers, John},
year = {1989},
month = {01},
pages = {227-243},
title = {An Empirical Comparison of Pruning Methods for Decision Tree Induction},
volume = {4},
journal = {Machine Learning},
doi = {10.1023/A:1022604100933}
}

@incollection{schaffer1992,
title = {Deconstructing the Digit Recognition Problem},
editor = {Derek Sleeman and Peter Edwards},
booktitle = {Machine Learning Proceedings 1992},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {394-399},
year = {1992},
isbn = {978-1-55860-247-2},
doi = {https://doi.org/10.1016/B978-1-55860-247-2.50056-5},
url = {https://www.sciencedirect.com/science/article/pii/B9781558602472500565},
author = {Cullen Schaffer},
abstract = {Decision tree pruning techniques and other forms of overfitting avoidance have often been considered statistical means of improving predictive accuracy. Intuitively, they are intended to determine the appropriate level of complexity for an induced model by distinguishing between signal and noise in training data, patterns that reflect the true underlying nature of data generation and those that arise by chance. In fact, however, overfitting avoidance methods simply encode preferences for certain classes of models. These preferences may increase accuracy when they bias induction in favor of predictive models, but they may just as well have the opposite effect if they bias induction away from predictive models. This paper analyzes the conditional value of overfitting avoidance and focuses on the effect of one highly-regarded pruning technique in variations of the well-known digit recognition problem.}
}

@article{brodley1995,
  title={Multivariate Decision Trees},
  author={Carla E. Brodley and Paul E. Utgoff},
  journal={Machine Learning},
  year={1995},
  volume={19},
  pages={45-77},
  url={https://api.semanticscholar.org/CorpusID:16836572}
}


@Inbook{kozial2009,
author="Koziol, Mariusz
and Wozniak, Michal",
editor="Kurzynski, Marek
and Wozniak, Michal",
title="Multivariate Decision Trees vs. Univariate Ones",
bookTitle="Computer Recognition Systems 3",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="275--284",
abstract="There is much current research into developing ever more efficient and accurate recognition algorithms. Decision tree classifiers are currently the focus of intense research. In this work methods of univariate and multivariate decision tree induction are presented and their qualities are compared via computer experiments. Additionally causes of decision tree parallelization are discussed.",
isbn="978-3-540-93905-4",
doi="10.1007/978-3-540-93905-4_33",
url="https://doi.org/10.1007/978-3-540-93905-4_33"
}

@article{friedl1997,
title = {Decision tree classification of land cover from remotely sensed data},
journal = {Remote Sensing of Environment},
volume = {61},
number = {3},
pages = {399-409},
year = {1997},
issn = {0034-4257},
doi = {https://doi.org/10.1016/S0034-4257(97)00049-7},
url = {https://www.sciencedirect.com/science/article/pii/S0034425797000497},
author = {M.A. Friedl and C.E. Brodley},
abstract = {Decision tree classification algorithms have significant potential for land cover mapping problems and have not been tested in detail by the remote sensing community relative to more conventional pattern recognition techniques such as maximum likelihood classification. In this paper, we present several types of decision tree classification algorithms arid evaluate them on three different remote sensing data sets. The decision tree classification algorithms tested include an univariate decision tree, a multivariate decision tree, and a hybrid decision tree capable of including several different types of classification algorithms within a single decision tree structure. Classification accuracies produced by each of these decision tree algorithms are compared with both maximum likelihood and linear discriminant function classifiers. Results from this analysis show that the decision tree algorithms consistently outperform the maximum likelihood and linear discriminant function classifiers in regard to classf — cation accuracy. In particular, the hybrid tree consistently produced the highest classification accuracies for the data sets tested. More generally, the results from this work show that decision trees have several advantages for remote sensing applications by virtue of their relatively simple, explicit, and intuitive classification structure. Further, decision tree algorithms are strictly nonparametric and, therefore, make no assumptions regarding the distribution of input data, and are flexible and robust with respect to nonlinear and noisy relations among input features and class labels.}
}

@InProceedings{young1984,
  title={Recursive Estimation and Time-Series Analysis: An Introduction},
  author={Peter C. Young},
  year={1984},
  url={https://api.semanticscholar.org/CorpusID:60181335}
}

@article{gallant1986,
author="S. I. Gallant",
title="Optimal linear discriminants",
journal="Eighth International Conference on Pattern Recognition",
publisher="IEEE",
year="1986",
pages="849-852",
URL="https://cir.nii.ac.jp/crid/1573668924476144256"
}

@phdthesis{frean1990,
  title={Small nets and short paths: Optimising neural computation},
  year=1990,
  author={Frean, Marcus Roland}
}

@article{kittler1986,
author="Kittler, J.",
title="Feature Selection and Extraction",
journal="Handbook of Pattern Recognition and Image Processing",
publisher="Academic Press, Inc.",
year="1986",
pages="59-83",
URL="https://cir.nii.ac.jp/crid/1573950400671608448"
}

@InProceedings{huan1998,
author="Liu, Huan
and Setiono, Rudy",
editor="Arikawa, Setsuo
and Motoda, Hiroshi",
title="Feature Transformation and Multivariate Decision Tree Induction",
booktitle="Discovey Science",
year="1998",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="279--291",
abstract="Univariate decision trees (UDT's) have inherent problems of replication, repetition, and fragmentation. Multivariate decision trees (MDT's) have been proposed to overcome some of the problems. Close examination of the conventional ways of building MDT's, however, reveals that the fragmentation problem still persists. A novel approach is suggested to minimize the fragmentation problem by separating hyperplane search from decision tree building. This is achieved by feature transformation. Let the initial feature vector be x, the new feature vector after feature transformation T is y, i.e., y = T(x). We can obtain an MDTb y (1) building a UDT on y; and (2) replacing new features y at each node with the combinations of initial features x. We elaborate on the advantages of this approach, the details of T, and why it is expected to perform well. Experiments are conducted in order to confirm the analysis, and results are compared to those of C4.5, OC1, and CART",
isbn="978-3-540-49292-4"
}

@article{canete,
author = {Cañete, Leonardo and Monroy, Raúl and Medina-Pérez, Miguel},
year = {2021},
month = {08},
pages = {1-1},
title = {A Review and Experimental Comparison of Multivariate Decision Trees},
volume = {PP},
journal = {IEEE Access},
doi = {10.1109/ACCESS.2021.3102239}
}

@misc{cristianini2000,
  title={An introduction to support vector machines and other kernel-based learning methods},
  author={Cristianini, Nello},
  year={2000},
  publisher={Cambridge University Press}
}

@article{cortes1995,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@article{cervantes2020,
title = {A comprehensive survey on support vector machine classification: Applications, challenges and trends},
journal = {Neurocomputing},
volume = {408},
pages = {189-215},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.10.118},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220307153},
author = {Jair Cervantes and Farid Garcia-Lamont and Lisbeth Rodríguez-Mazahua and Asdrubal Lopez},
keywords = {SVM, Classification, Machine learning},
abstract = {In recent years, an enormous amount of research has been carried out on support vector machines (SVMs) and their application in several fields of science. SVMs are one of the most powerful and robust classification and regression algorithms in multiple fields of application. The SVM has been playing a significant role in pattern recognition which is an extensively popular and active research area among the researchers. Research in some fields where SVMs do not perform well has spurred development of other applications such as SVM for large data sets, SVM for multi classification and SVM for unbalanced data sets. Further, SVM has been integrated with other advanced methods such as evolve algorithms, to enhance the ability of classification and optimize parameters. SVM algorithms have gained recognition in research and applications in several scientific and engineering areas. This paper provides a brief introduction of SVMs, describes many applications and summarizes challenges and trends. Furthermore, limitations of SVMs will be identified. The future of SVMs will be discussed in conjunction with further applications. The applications of SVMs will be reviewed as well, especially in the some fields.}
}

@book{vapnik2013,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}

@article{dong2005,
  title={Fast SVM training algorithm with decomposition on very large data sets},
  author={Dong, Jian-xiong and Krzyzak, Adam and Suen, Ching Y},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={27},
  number={4},
  pages={603--618},
  year={2005},
  publisher={IEEE}
}

@article{hsu2002,
  title={A comparison of methods for multiclass support vector machines},
  author={Hsu, Chih-Wei and Lin, Chih-Jen},
  journal={IEEE transactions on Neural Networks},
  volume={13},
  number={2},
  pages={415--425},
  year={2002},
  publisher={IEEE}
}

@misc{nanda2018,
  title={A Comparison Study of Kernel Functions in the Support Vector Machine and A Comparison Study of Kernel Functions in the Support Vector Machine and Its Application for Termite Detection},
  author={Nanda, MA and Seminar, KB and Nandika, D and Maddu, A},
  year={2018},
  publisher={September). https://doi. org/10.3390/info9010005}
}