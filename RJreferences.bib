@InProceedings{stree2021,
author="Monta{\~{n}}ana, Ricardo
and G{\'a}mez, Jose A.
and Puerta, Jose M.",
editor="Alba, Enrique
and Luque, Gabriel
and Chicano, Francisco
and Cotta, Carlos
and Camacho, David
and Ojeda-Aciego, Manuel
and Montes, Susana
and Troncoso, Alicia
and Riquelme, Jos{\'e}
and Gil-Merino, Rodrigo",
title="STree: A Single Multi-class Oblique Decision Tree Based on Support Vector Machines",
booktitle="Advances in Artificial Intelligence",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="54--64",
abstract="We propose a new oblique decision tree algorithm based on support vector machines. Our algorithm produces a single model for a multi-class target variable. On the contrary to previous works that manage the multi-class problem by using clustering at each split, we test all the one-vs-rest labels at each split, choosing the one which minimizes an impurity measure. The experimental evaluation carried out over 49 datasets shows that our algorithm is ranked before those used for comparison, and significantly outperforms all of them when the SVM hyperparameters are carefully tuned.",
isbn="978-3-030-85713-4"
}

@article{friedman2006,
  title={A projection pursuit algorithm for exploratory data analysis},
  author={Friedman, Jerome H and Tukey, John W},
  journal={IEEE Transactions on computers},
  volume={100},
  number={9},
  pages={881--890},
  year={2006},
  publisher={IEEE}
}@

InProceedings{kruskal1969,
  title={Toward a practical method which helps uncover the structure of a set of multivariate observations by finding the linear transformation which optimizes a new “index of condensation”},
  author={Kruskal, Joseph B},
  booktitle={Statistical computation},
  pages={427--440},
  year={1969},
  organization={Elsevier}
}

@article{pptree2013,
title = "PPtree: Projection pursuit classification tree",
abstract = "In this paper, we propose a new classification tree, the projection pursuit classification tree (PPtree). It combines tree structured methods with projection pursuit dimension reduction. This tree is originated from the projection pursuit method for classification. In each node, one of the projection pursuit indices using class information - LDA, Lr or PDA indices - is maximized to find the projection with the most separated group view. On this optimized data projection, the tree splitting criteria are applied to separate the groups. These steps are iterated until the last two classes are separated. The main advantages of this tree is that it effectively uses correlation between variables to find separations, and it has visual representation of the differences between groups in a 1-dimensional space that can be used to interpret results. Also in each node of the tree, the projection coefficients represent the variable importance for the group separation. This information is very helpful to select variables in classification problems.",
author = "Lee, \{Yoon Dong\} and Cook, \{Dianne Helen\} and Jiwon Park and Eun-Kyung Lee",
year = "2013",
doi = "10.1214/13-EJS810",
language = "English",
volume = "7",
pages = "1369 -- 1386",
journal = "Electronic Journal of Statistics",
issn = "1935-7524",
publisher = "Institute of Mathematical Statistics",
number = "1",
}

@article{lopez2018,
author = {Rivera-Lopez, Rafael and Canul-Reich, Juana},
year = {2018},
month = {01},
pages = {1-1},
title = {Construction of Near-Optimal Axis-Parallel Decision Trees Using a Differential-Evolution-Based Approach},
volume = {PP},
journal = {IEEE Access},
doi = {10.1109/ACCESS.2017.2788700}
}

@article{pagallo1990,
  title={Boolean Feature Discovery in Empirical Learning},
  author={Giulia Pagallo and David Haussler},
  journal={Machine Learning},
  year={1990},
  volume={5},
  pages={71-99},
  url={https://api.semanticscholar.org/CorpusID:5661437}
}

@book{breiman1984,
  author = {Breiman, Leo and Friedman, J. H. and Olshen, R. A. and Stone, C. J.},
  publisher = {Wadsworth},
  title = {Classification and Regression Trees.},
  url = {http://lyle.smu.edu/~mhd/8331f06/cart.pdf},
  year = 1984
}

@article{quinlan1986,
  title={Induction of Decision Trees},
  author={J. Ross Quinlan},
  journal={Machine Learning},
  year={1986},
  volume={1},
  pages={81-106},
  url={https://api.semanticscholar.org/CorpusID:189902138}
}

@book{quinlan1993,
  title={C4. 5: programs for machine learning},
  author={Quinlan, J Ross},
  year={2014},
  publisher={Elsevier}
}

@article{mingers1989,
author = {Mingers, John},
year = {1989},
month = {01},
pages = {227-243},
title = {An Empirical Comparison of Pruning Methods for Decision Tree Induction},
volume = {4},
journal = {Machine Learning},
doi = {10.1023/A:1022604100933}
}

@incollection{schaffer1992,
title = {Deconstructing the Digit Recognition Problem},
editor = {Derek Sleeman and Peter Edwards},
booktitle = {Machine Learning Proceedings 1992},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {394-399},
year = {1992},
isbn = {978-1-55860-247-2},
doi = {https://doi.org/10.1016/B978-1-55860-247-2.50056-5},
url = {https://www.sciencedirect.com/science/article/pii/B9781558602472500565},
author = {Cullen Schaffer},
abstract = {Decision tree pruning techniques and other forms of overfitting avoidance have often been considered statistical means of improving predictive accuracy. Intuitively, they are intended to determine the appropriate level of complexity for an induced model by distinguishing between signal and noise in training data, patterns that reflect the true underlying nature of data generation and those that arise by chance. In fact, however, overfitting avoidance methods simply encode preferences for certain classes of models. These preferences may increase accuracy when they bias induction in favor of predictive models, but they may just as well have the opposite effect if they bias induction away from predictive models. This paper analyzes the conditional value of overfitting avoidance and focuses on the effect of one highly-regarded pruning technique in variations of the well-known digit recognition problem.}
}

@article{brodley1995,
  title={Multivariate Decision Trees},
  author={Carla E. Brodley and Paul E. Utgoff},
  journal={Machine Learning},
  year={1995},
  volume={19},
  pages={45-77},
  url={https://api.semanticscholar.org/CorpusID:16836572}
}


@Inbook{kozial2009,
author="Koziol, Mariusz
and Wozniak, Michal",
editor="Kurzynski, Marek
and Wozniak, Michal",
title="Multivariate Decision Trees vs. Univariate Ones",
bookTitle="Computer Recognition Systems 3",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="275--284",
abstract="There is much current research into developing ever more efficient and accurate recognition algorithms. Decision tree classifiers are currently the focus of intense research. In this work methods of univariate and multivariate decision tree induction are presented and their qualities are compared via computer experiments. Additionally causes of decision tree parallelization are discussed.",
isbn="978-3-540-93905-4",
doi="10.1007/978-3-540-93905-4_33",
url="https://doi.org/10.1007/978-3-540-93905-4_33"
}

@article{friedl1997,
title = {Decision tree classification of land cover from remotely sensed data},
journal = {Remote Sensing of Environment},
volume = {61},
number = {3},
pages = {399-409},
year = {1997},
issn = {0034-4257},
doi = {https://doi.org/10.1016/S0034-4257(97)00049-7},
url = {https://www.sciencedirect.com/science/article/pii/S0034425797000497},
author = {M.A. Friedl and C.E. Brodley},
abstract = {Decision tree classification algorithms have significant potential for land cover mapping problems and have not been tested in detail by the remote sensing community relative to more conventional pattern recognition techniques such as maximum likelihood classification. In this paper, we present several types of decision tree classification algorithms arid evaluate them on three different remote sensing data sets. The decision tree classification algorithms tested include an univariate decision tree, a multivariate decision tree, and a hybrid decision tree capable of including several different types of classification algorithms within a single decision tree structure. Classification accuracies produced by each of these decision tree algorithms are compared with both maximum likelihood and linear discriminant function classifiers. Results from this analysis show that the decision tree algorithms consistently outperform the maximum likelihood and linear discriminant function classifiers in regard to classf — cation accuracy. In particular, the hybrid tree consistently produced the highest classification accuracies for the data sets tested. More generally, the results from this work show that decision trees have several advantages for remote sensing applications by virtue of their relatively simple, explicit, and intuitive classification structure. Further, decision tree algorithms are strictly nonparametric and, therefore, make no assumptions regarding the distribution of input data, and are flexible and robust with respect to nonlinear and noisy relations among input features and class labels.}
}

@InProceedings{young1984,
  title={Recursive Estimation and Time-Series Analysis: An Introduction},
  author={Peter C. Young},
  year={1984},
  url={https://api.semanticscholar.org/CorpusID:60181335}
}

@article{gallant1986,
author="S. I. Gallant",
title="Optimal linear discriminants",
journal="Eighth International Conference on Pattern Recognition",
publisher="IEEE",
year="1986",
pages="849-852",
URL="https://cir.nii.ac.jp/crid/1573668924476144256"
}

@phdthesis{frean1990,
  title={Small nets and short paths: Optimising neural computation},
  year=1990,
  author={Frean, Marcus Roland}
}

@article{kittler1986,
author="Kittler, J.",
title="Feature Selection and Extraction",
journal="Handbook of Pattern Recognition and Image Processing",
publisher="Academic Press, Inc.",
year="1986",
pages="59-83",
URL="https://cir.nii.ac.jp/crid/1573950400671608448"
}

@InProceedings{huan1998,
author="Liu, Huan
and Setiono, Rudy",
editor="Arikawa, Setsuo
and Motoda, Hiroshi",
title="Feature Transformation and Multivariate Decision Tree Induction",
booktitle="Discovey Science",
year="1998",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="279--291",
abstract="Univariate decision trees (UDT's) have inherent problems of replication, repetition, and fragmentation. Multivariate decision trees (MDT's) have been proposed to overcome some of the problems. Close examination of the conventional ways of building MDT's, however, reveals that the fragmentation problem still persists. A novel approach is suggested to minimize the fragmentation problem by separating hyperplane search from decision tree building. This is achieved by feature transformation. Let the initial feature vector be x, the new feature vector after feature transformation T is y, i.e., y = T(x). We can obtain an MDTb y (1) building a UDT on y; and (2) replacing new features y at each node with the combinations of initial features x. We elaborate on the advantages of this approach, the details of T, and why it is expected to perform well. Experiments are conducted in order to confirm the analysis, and results are compared to those of C4.5, OC1, and CART",
isbn="978-3-540-49292-4"
}

@article{canete,
author = {Cañete, Leonardo and Monroy, Raúl and Medina-Pérez, Miguel},
year = {2021},
month = {08},
pages = {1-1},
title = {A Review and Experimental Comparison of Multivariate Decision Trees},
volume = {PP},
journal = {IEEE Access},
doi = {10.1109/ACCESS.2021.3102239}
}

@misc{cristianini2000,
  title={An introduction to support vector machines and other kernel-based learning methods},
  author={Cristianini, Nello},
  year={2000},
  publisher={Cambridge University Press}
}

@article{cortes1995,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@article{cervantes2020,
title = {A comprehensive survey on support vector machine classification: Applications, challenges and trends},
journal = {Neurocomputing},
volume = {408},
pages = {189-215},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.10.118},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220307153},
author = {Jair Cervantes and Farid Garcia-Lamont and Lisbeth Rodríguez-Mazahua and Asdrubal Lopez},
keywords = {SVM, Classification, Machine learning},
abstract = {In recent years, an enormous amount of research has been carried out on support vector machines (SVMs) and their application in several fields of science. SVMs are one of the most powerful and robust classification and regression algorithms in multiple fields of application. The SVM has been playing a significant role in pattern recognition which is an extensively popular and active research area among the researchers. Research in some fields where SVMs do not perform well has spurred development of other applications such as SVM for large data sets, SVM for multi classification and SVM for unbalanced data sets. Further, SVM has been integrated with other advanced methods such as evolve algorithms, to enhance the ability of classification and optimize parameters. SVM algorithms have gained recognition in research and applications in several scientific and engineering areas. This paper provides a brief introduction of SVMs, describes many applications and summarizes challenges and trends. Furthermore, limitations of SVMs will be identified. The future of SVMs will be discussed in conjunction with further applications. The applications of SVMs will be reviewed as well, especially in the some fields.}
}

@book{vapnik2013,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}

@article{dong2005,
  title={Fast SVM training algorithm with decomposition on very large data sets},
  author={Dong, Jian-xiong and Krzyzak, Adam and Suen, Ching Y},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={27},
  number={4},
  pages={603--618},
  year={2005},
  publisher={IEEE}
}

@article{hsu2002,
  title={A comparison of methods for multiclass support vector machines},
  author={Hsu, Chih-Wei and Lin, Chih-Jen},
  journal={IEEE transactions on Neural Networks},
  volume={13},
  number={2},
  pages={415--425},
  year={2002},
  publisher={IEEE}
}

@misc{nanda2018,
  title={A Comparison Study of Kernel Functions in the Support Vector Machine and A Comparison Study of Kernel Functions in the Support Vector Machine and Its Application for Termite Detection},
  author={Nanda, MA and Seminar, KB and Nandika, D and Maddu, A},
  year={2018},
  publisher={September). https://doi. org/10.3390/info9010005}
}

@article{zhang2007,
author = {Zhang, Li and Zhou, Weida and Su, Tian-Tian and Jiao, Licheng},
year = {2007},
month = {02},
pages = {1-16},
title = {Decision Tree Support Vector Machine.},
volume = {16},
journal = {International Journal on Artificial Intelligence Tools},
doi = {10.1142/S0218213007003163}
}

@InProceedings{bennet1998,
  author={Bennett, K.P. and Blue, J.A.},
  booktitle={1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36227)}, 
  title={A support vector machine approach to decision trees}, 
  year={1998},
  volume={3},
  number={},
  pages={2396-2401 vol.3},
  keywords={Support vector machines;Decision trees;Support vector machine classification;Polynomials;Neural networks;Risk management;Statistical learning;Ear;Kernel;Radial basis function networks},
  doi={10.1109/IJCNN.1998.687237}
  }
  
@InProceedings{takahashi2002,
author = {Takahashi, Fumitake and Abe, Shigeo},
year = {2002},
month = {12},
pages = {1418 - 1422 vol.3},
title = {Decision-tree-based multiclass support vector machines},
volume = {3},
isbn = {981-04-7524-1},
journal = {Proceedings of the 9th International Conference on Neural Information Processing},
doi = {10.1109/ICONIP.2002.1202854}
}

@article{bala2011,
  title={Optimal decision tree based multi-class support vector machine},
  author={Bala, Manju and Agrawal, RK},
  journal={Informatica},
  volume={35},
  number={2},
  year={2011}
}

@ARTICLE{Mangasarian2006,
  author={Mangasarian, O.L. and Wild, E.W.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Multisurface proximal support vector machine classification via generalized eigenvalues}, 
  year={2006},
  volume={28},
  number={1},
  pages={69-74},
  keywords={Support vector machine classification;Eigenvalues and eigenfunctions;Kernel;Testing;Support vector machines;MATLAB;Linear algebra;Proteins;Software standards;Parallel processing;Index Terms- Support vector machines;proximal classification;generalized eigenvalues.},
  doi={10.1109/TPAMI.2006.17}}
  
@article{ganaie2022,
title = {Oblique and rotation double random forest},
journal = {Neural Networks},
volume = {153},
pages = {496-517},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022002258},
author = {M.A. Ganaie and M. Tanveer and P.N. Suganthan and V. Snasel},
keywords = {Double random forest, Oblique random forest, Ensemble learning, Bootstrap, Decision tree, classification},
abstract = {Random Forest is an ensemble of decision trees based on the bagging and random subspace concepts. As suggested by Breiman, the strength of unstable learners and the diversity among them are the ensemble models’ core strength. In this paper, we propose two approaches known as oblique and rotation double random forests. In the first approach, we propose rotation based double random forest. In rotation based double random forests, transformation or rotation of the feature space is generated at each node. At each node different random feature subspace is chosen for evaluation, hence the transformation at each node is different. Different transformations result in better diversity among the base learners and hence, better generalization performance. With the double random forest as base learner, the data at each node is transformed via two different transformations namely, principal component analysis and linear discriminant analysis. In the second approach, we propose oblique double random forest. Decision trees in random forest and double random forest are univariate, and this results in the generation of axis parallel split which fails to capture the geometric structure of the data. Also, the standard random forest may not grow sufficiently large decision trees resulting in suboptimal performance. To capture the geometric properties and to grow the decision trees of sufficient depth, we propose oblique double random forest. The oblique double random forest models are multivariate decision trees. At each non-leaf node, multisurface proximal support vector machine generates the optimal plane for better generalization performance. Also, different regularization techniques (Tikhonov regularization, axis-parallel split regularization, Null space regularization) are employed for tackling the small sample size problems in the decision trees of oblique double random forest. The proposed ensembles of decision trees produce trees with bigger size compared to the standard ensembles of decision trees as bagging is used at each non-leaf node which results in improved performance. The evaluation of the baseline models and the proposed oblique and rotation double random forest models is performed on benchmark 121 UCI datasets and real-world fisheries datasets. Both statistical analysis and the experimental results demonstrate the efficacy of the proposed oblique and rotation double random forest models compared to the baseline models on the benchmark datasets.}
}

@article{montanana2025,
title = {ODTE—An ensemble of multi-class SVM-based oblique decision trees},
journal = {Expert Systems with Applications},
volume = {273},
pages = {126833},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126833},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425004555},
author = {Ricardo Montañana and José A. Gámez and José M. Puerta},
keywords = {Oblique decision trees, Supervised classification, SVM, Ensemble, Multiclass strategies},
abstract = {We propose ODTE, a new ensemble that uses oblique decision trees as base classifiers. Additionally, we introduce STree, the base algorithm for growing oblique decision trees, which leverages support vector machines to define hyperplanes within the decision nodes. We embed a multiclass strategy (one-vs-one or one-vs-rest) at the decision nodes, allowing the model to directly handle non-binary classification tasks without the need to cluster instances into two groups, as is common in other approaches from the literature. In each decision node, only the best-performing model (SVM)—the one that minimizes an impurity measure for the n-ary classification—is retained, even if the learned SVM addresses a binary classification subtask. An extensive experimental study involving 49 datasets and various state-of-the-art algorithms for oblique decision tree ensembles has been conducted. Our results show that ODTE ranks consistently above its competitors, achieving significant performance gains when hyperparameters are carefully tuned. Moreover, the oblique decision trees learned through STree are more compact than those produced by other algorithms evaluated in our experiments.}
}


@InProceedings{Street1993,
  title={Nuclear feature extraction for breast tumor diagnosis},
  author={William Nick Street and William H. Wolberg and Olvi L. Mangasarian},
  booktitle={Electronic imaging},
  year={1993},
  url={https://api.semanticscholar.org/CorpusID:14922543}
}

@Software{Montanana2021,
  author    = {R. Montañana and J. A. Gámez and J. M. Puerta},
  title     = {STree: a single multi-class oblique decision tree based on support vector machines},
  booktitle = {Lecture Notes in Computer Science (LNAI), 12882},
  pages     = {54--64},
  year      = {2021},
  publisher = {Springer}
}


@InProceedings{menkovski2008,
  author={Menkovski, Vlado and Christou, Ioannis T. and Efremidis, Sofoklis},
  booktitle={2008 7th IEEE International Conference on Cybernetic Intelligent Systems}, 
  title={Oblique Decision Trees using embedded Support Vector Machines in classifier ensembles}, 
  year={2008},
  volume={},
  number={},
  pages={1-6},
  keywords={Decision trees;Support vector machines;Support vector machine classification;Classification tree analysis;Testing;Machine learning;Machine learning algorithms;Entropy;Information technology;Working environment noise;Classification;Decision Trees;On-line Learning;Supervised Learning},
  doi={10.1109/UKRICIS.2008.4798937}
  }
  
@article{scikit2011,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@article{crammer2001,
  title={On the algorithmic implementation of multiclass kernel-based vector machines},
  author={Crammer, Koby and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={2},
  number={Dec},
  pages={265--292},
  year={2001}
}

@article{said2025,
author = {Chabbouh, Marwa and Bechikh, Slim and Mezura-Montes, Efrén and Ben Said, Lamjed},
year = {2025},
month = {01},
pages = {},
title = {Evolutionary optimization of the area under precision-recall curve for classifying imbalanced multi-class data},
volume = {31},
journal = {Journal of Heuristics},
doi = {10.1007/s10732-024-09544-z}
}
